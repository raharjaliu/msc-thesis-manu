\documentclass[pdftex,12pt,a4paper]{report}

\usepackage[pdftex]{graphicx}
\usepackage[ansinew]{inputenc}
\usepackage{geometry}
\usepackage{bbold}
\usepackage{program}
\usepackage[toc,page]{appendix}
\usepackage{subcaption}
\usepackage{url}
\usepackage{textcomp}
\usepackage{gensymb}
% float forces latex to place image right "HERE"
\usepackage{float}
% math utility package
\usepackage{savesym}
\savesymbol{implies}
\savesymbol{overset}
\usepackage{amsmath}
\usepackage{mathtools}
\savesymbol{lneq}
\usepackage{amssymb}
\usepackage[]{algorithm2e}
\usepackage{lscape}
\usepackage{graphicx}
\usepackage{hyperref}
\geometry{a4paper,left=2.5cm,right=2.5cm, top=2.5cm, bottom=3cm}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% definitions for pseudocode
\newcommand{\forcond}{$i=0$ \KwTo $k - 1$}

% declare macros
%\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
%\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% definition for ToC

\usepackage{lipsum}% http://ctan.org/pkg/lipsum
\usepackage{titletoc}% http://ctan.org/pkg/titletoc
\titlecontents*{chapter}% <section-type>
  [0pt]% <left>
  {}% <above-code>
  {\bfseries\chaptername\ \thecontentslabel\quad}% <numbered-entry-format>
  {}% <numberless-entry-format>
  {\bfseries\hfill\contentspage}% <filler-page-format>

\begin{document}
\begin{titlepage}

%%LR
\sffamily

\begin{center}


% Oberer Teil der Titelseite:
\includegraphics[width=0.3\textwidth]{logo2.jpg}
\hfill
\includegraphics[width=0.4\textwidth]{logo1.jpg}  
\\[5cm]

{\Large Department of Mathematics}\\[0.5cm]
{\Large Chair of Mathematical Modeling of Biological Systems}\\[0.5cm]
{Technische Universit\"at M\"unchen}\\[2cm]
{\Large Master's Thesis in Bioinformatics}\\[1.5cm]

% Title
\HRule \\[0.4cm]
{ \huge \bfseries Single-cell analysis of cancer drug response using computer vision and learning algorithms on time-lapse micro-trench data}\\[0.4cm]

\HRule \\[1.5cm]

{\Large Pandu Raharja}\\[2.5cm]

\vfill
\end{center}
\end{titlepage}
%% THIS REMOVES PAGE NUMBER
%%\pagestyle{empty}

%%LR comprehensive title
\begin{titlepage}
{\sffamily


\begin{center}
\includegraphics[width=0.3\textwidth]{logo2.jpg}
\hfill
\includegraphics[width=0.4\textwidth]{logo1.jpg}  
\\[1.5cm]  

{\Large Department of Mathematics}\\[0.5cm]
{\Large Chair of Mathematical Modeling of Biological Systems}\\[0.5cm]
{Technische Universit\"at M\"unchen}\\[1cm]

{\Large Master's Thesis in Bioinformatics}\\[2cm]
{\textbf{\Large Single-cell analysis of cancer drug response using computer vision and learning algorithms on time-lapse micro-trench data}}\\[2cm]
{\textbf{\Large Wirkungsanalyse von Krebsmedikamenten in Einzeller Aufl\"osung durch die Anwendung von Computer-Vision- und Machine-Learning-Algorithmen auf Microtrench- Videoaufnahme}}\\[4cm]

\end{center}
\begin{center}\Large
  \begin{tabular}{ll}
    Author:& Pandu Raharja\\
    Supervisor: &  Prof. Dr. Fabian Theis, Dr. Carsten Marr\\
    Advisor:        &  Prof. Dr. Fabian Theis\\
    & Prof. Dr. Dmitrij Frishman\\
    Submitted:     &  15.12.2017
  \end{tabular}
\end{center}

}% end title page

\end{titlepage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% thesis content starts here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\parindent 0cm
%%%%%%%%%%%%%%%%%%%%%%%%%%deutsche Version%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
  \section*{Selbst\"andigkeitserkl\"arung}
  Ich erkl\"are hiermit, dass ich die vorliegende Arbeit selbst\"andig verfasst 
  und nur unter Verwendung der angegebenen Quellen und Hilfsmittel angefertigt habe. 
  Weiterhin erkl\"are ich, eine Diplomarbeit in diesem Studiengebiet erstmalig einzureichen.\\
  \vspace{3\baselineskip}
  
  M\"unchen, den \today \hspace{0.1\linewidth}\parbox{0.3\linewidth}{\dotfill}

%%%%%%%%%%%%%%%%%%%%%%%%%%englische Version%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Statement of authorship}
I declare that I completed this thesis on my own and that information which has been 
directly or indirectly taken from other sources has been noted as such. Neither this 
nor a similar work has been presented to an examination committee.

  \vspace{3\baselineskip}
  
  Munich, \today \hspace{0.1\linewidth}\parbox{0.3\linewidth}{\dotfill}
}

\newpage

\chapter*{Acknowledgement}

First and foremost, I would like to thank mom and dad, for the support even from faraway lands. Even though we are continents apart, your pray and hope will be always with me.\\

To Jennifer Carissa, whose support has made it possible to go through the hard parts of my life in the last years. This thesis is only possible because of you.\\

To Julian, Ganesh and other best friends which I resort to for personal assistance. Friends really are hard to find.\\

And finally, to Prof. Fabian Theis, Prof. Joachim R\"adler, Elisavet Chatzopoulou, Felix Buggenthin and Alexandra Murschhauser for tremendous assistant in doing this project. Never did I know a project could be done in this maner.\\

Finally for my best supervisor, Carste Marr. Your boundless patience, deep insight and attention to detail have made this project impossible without your selfless assistance. I hope we could do more things in the future. 

\newpage

\chapter*{Terminology and abbreviations}

{
\flushleft

Following terms are used frequently in this thesis and therefore merit special explanation:

\vspace{10mm}

\begingroup
%\setlength{\tabcolsep}{10pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.5} % Default value: 1
\begin{tabular}{ l l }
\textbf{Name} & \textbf{Explanation} \\
micro-trench & Single unit of micro-trench inside a well \\
wafer & The plate where the microtrench is manufactured on\\
well & Macro-containment where wafer is located, usually part of larger wells\\
& installation (2 by 4 wells in our case).\\
slice & An image \\
stack & An ordered sequence of images (usually coming from the same channel) \\
channel & A connection between data source (camera) and storage. Images coming\\
 & from the same channel have same capture properties formatting.
\end{tabular}
\endgroup

\vspace{10mm}

Note that in this thesis, following things are used interchangeably:

\vspace{10mm}

\begin{itemize}
\item image and slice, while referring to an image particularly a capture image of the trenches and its processed versions.
\item image and figure, while referring to an image shown in this paper in general.
\end{itemize}

\vspace{10mm}

\begingroup
%\setlength{\tabcolsep}{10pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.5} % Default value: 1
\begin{tabular}{ l l }
\textbf{Symbol} & \textbf{Explanation} \\
$B_t$ & A 1-bit monochrome image taken at time t (t-th image)\\
$G_t$ & Grayscale image at time $t$\\
$\mathbb{G}_{x, y}$ & Sobel gradient of pixel $M_{t, x, y}$\\
$G(x)$ & One dimensional Gaussian function\\
$G(x, y)$ & Two dimensional Gaussian function\\
$I_t(x, y)$ & Intensity function for pixel $(x, y)$ of RGB image taken at time t\\
$J_t(x, y)$ & Brightness and contrast transforming function for pixel $(x, y)$ of grayscale image taken at time t\\
$M$ & A stack of images \\
$M_t$ & An RGB image captured at time t \\
$M_{t, x, y}$ & Pixel at $(x, y)$ in $M_t$ \\
$M_{t, x, y, 1}$ & First channel of pixel $M_{t, x, y}$ \\
$M^b_t$ & A $b$-bit encoded RGB image captured at time t\\
\end{tabular}
\endgroup

}
\newpage

\begin{abstract}
Quantitative measurement of cancer drug response is esential to objectively gauge the efficacy of cancer drugs. So far, there has been no method to track and  quantitatively measure single-cell response of of cancer drug treatment. A novel pipeline is presented in this thesis. First, a quasi-high-throughput method to track cells and quantitatively analyze single-cell response to drugs. We investigate the response of model cancer cell lineagues, MOLM and Jurkat, to known anti-cancer drugs Vincristine and Doxorubicine. While the method enabled relatively easy and quasi-high-throughput analysis of cancer treatment \textit{in vitro}, our pipeline could also be adapted in varios contexts involving single-cell analysis with reasonable amount of modifications necessary.
\end{abstract}

\newpage

\tableofcontents

\newpage

\chapter{Introduction}

Cancer is among the deadliest diseases ever known to human being. It is a leading cause of death in 2009, second only to cardiovascular diseases \cite{sudhakar2009history}. The numbers are discontenting, especially in the developed world. In the United States alone, half of men and a third of women are expected to develop some kind of cancer. According to US government, in 2016 alone, an estimated 1,685,210 people will be diagnosed with cancer, while 595,690 more will be die from it \cite{cancergov2017stat}.

Worldwide, the International Agency for Research on Cancer's GLOBOCAN series report that, in 2014, \cite{ferlay2015cancer}.

\begin{verbatim}
I.
- readable to people without background in the fields
- non technical at all
II.
- what have the researches done
-- biologics
-- technicals
III.
thesis overview
4~8 pages
\end{verbatim}

\section{Variability of cell-to-cell response towards cancer treatment}

%TODO : modify to local use

Cell-to-cell variability in response to external stimuli is a pervasive trait in cellular systems that prevails even in isogenic cell populations. Here, heterogeneity might be caused by epigenetic modifications, differences in the cell cycle phase, or by intrinsic stochastic fluctuations in gene expression and biochemical regulation. The implications of heterogeneity for cancer progression and treatment are poorly understood. In some cases, heterogeneity is dominated by intrinsic fluctuations in the stochastic expression of key regulators that randomly alter the sensitiveness of individual cancer cells. A raw model of this scenario has been put forward in recent work on TRAIL-induced apoptosis [1, 2]. Experiments and simulations show that variability in cell fate is sensitive to small stochastic increases in the levels of Bcl-2 and are transiently heritable to siblings [3, 4]. The study on TRAIL-induced apoptosis led to a novel interpretation of fractional killing and predicts reversible resistance to chemotherapy. The profound consequences for cancer treatment have been subject to theoretical studies on the stochastic origins of cell-to-cell variability in cancer cell death decisions [5-7]. 
Cancer is an intrinsically highly diverse disease; tumors of any different histological type not only exhibit genetic diversity but also display their variation when exposed to all forms of chemotherapy [8]. Most state of the art chemotherapeutic drugs in clinical use, target rapidly dividing cells and trigger apoptosis. Vincristine (VCR), an antitumor vinca alkaloid, binds to tubulin and stops the dividing cell from separating its chromosomes during metaphase in M-phase. It is thus considered an M-phase dependent drug [9]. In contrast, daunorubicin, an anthracycline aminoglycoside antineoplastic, intercalates on DNA and inhibits the function of the enzyme topoisomerase II during transcription and replication. Daunorubicin is thus expected to act throughout the whole cell cycle and but especially strong during the DNA replication in the S-phase. Both drugs are used to fight hematopoietic disorders such as Acute Myeloid Leukemia (AML)[10, 11] among other neoplams. In the literature, a drug that interferes with the cell cycle is in general considered cell cycle dependent, but in practice it is not clear whether it only acts on a specific cell cycle phase, due to side effect toxicities. Based on this, it is hypothesized that both VCR and daunorubicin are cell cycle dependent. Since it is assumed that VCR acts only during M-phase in their cycling, we can expect that cells that are closer to M-phase will die earlier [12, 13].  
It is essential toTo understand sources of heterogeneous response of to cancer in therapy and , in order to design novel therapeutic strategies and potent agents,  that not only targets key signaling pathways with high specificity but also address the contextual role of cell cycle timing in cancer therapyin the response of cells to chemotherapeutic drugs has to be investigated. In this context, time-lapse imaging, which allows for recording accurate histories of individual cancer cell fates and cancer subpopulations, received increasing attention [12, 14, 15]. However, the to study the effect of particular cell cycle phases on chemo sensitiveness, single cells have to be observed continuously throughout division, drug addition, and death.  has not been explored at the single cell level. 
The typical bottleneck for a high-throughput analysis for such is cell tracking. Tracking single cells in time-lapse microscopy movies is a challenging problem. Different automatic image analysis tracking tools have been proposed [19,20 ] and compared [16,17]  [16-20]  but to achieve tracks for for fast moving cells movements, high cell densities, challenging cell identification, and long-term observations, also time-intensive manual tracking becomes necessary to achieve accurate tracksis often applied [18,21] to generate tracks with maximal accuracy. For many approaches, the workload of manual tracking has to be compared to correcting erroneous track from tracking algorithms [schröder review].   [21] . Confining cells spatially obviously reduces possible tracking errors and facilitates the application of tracking algorithms. In particular, non-adherent cells are painstaking to track since diffusive and convective drift in long-term measurements complicate cell assignments [20]. Among the techniques to capture non-adherent cells for long-term observation microfluidic devices [22] as well as micro-well platforms [23-28] were have been developed. Alternatively, individual cell cycle phases were imaged using fluorescent cell cycle reporters [29].  The confinement of single cells into well-defined spatial structures provides a straightforward implementation to facilitate automatic tracking since long-term crossing of individual tracks is avoided. PConsequently, platforms that confine single starting cells and thus lead to spatially separated cell families (also called clones) are an especially derived from a single mother cell are a useful tool to investigate cell cycle length, sister cell correlations, or the effect of cell cycle phases to to enable the automatization of the image analysis and to yield in a faster and effortless way to collect time traces that can address questions regarding the sources of cell-to-cell variability in a high-throughput manner. 
Here we introduce a platform that enables the continuous observation of cell families derived from individual non-adherent cells of the leukemia cell line MOLM-13. The platform employs arrays of micro-trenches optimized to observe cells for two consecutive generations. We demonstrate that automated image analysis is feasible and allows for precisely determination determiningof the cell division cycle time  distribution and sister cell correlations. A key feature of the platform is the direct and parallel observation of many hundreds of cells with individualin different cell cycle statess. We show that the time-to-death after induction of apoptosis of the leukemia cell line MOLM-13 using the anti-mitotic agent vincristine (VCR) and daunorubicin (dauno) has a small dependence on cell cycle . The results were found consistent with experiments using cells that were synchronized using standard thymidine cell cycle arrest. Using arrays of micro-trenchesOur approach also enabled a time correlation analysis which showed that the time-to-death of daughter cells correlates with the time spent in cell cycle, while this effect is not detectable when cells are synchronized. 


\section{Hypothesis}

In this part, the hypothesis underlying the experiment is presented.

\section{Advances in microfluidics, image processing and machine learning}

In the recent decades, as in many other fields, there are numerous groundbreaking advances in the fields of microfluidics, image processing and machine learning.

%TODO : continue

The advances, coupled with general technological advances in computing power and energy efficiency, have made it possible for us to design almost completely automated analytical pipeline for single-cell microfluidic system.

%TODO : expand

While not all methods and algorithms in this project are the most recent, many of them are state-of-the-art and/or good enough for the pipeline to work seamlessly.

\subsection{Microfluidics}

As the name suggests, 'microfluidics' concerns the manipulation of fluids in a small working dimension, typically starting from nanometers to lower millimeters \cite{whitesides2006origins}. In modern context, the entire fields tries to find application of micro-sized devices which hold and control the state of liquid \cite{whitesides2006origins} including cell culture medium. There are two categories of microfluidics: active and passive microfluidics devices. The separation is based on the device's ability to actively manipulate the flow and control of devices \cite{sekhavati2015dynamic}. Active devices use micro-valves to perform sophisticated chemical processes \cite{marsden1993interdisciplinary}. This goes as far as reactions at individual cell level \cite{eyer2012microchamber}. A passive device, which our micro-array system is, exploits on the other hand its physical property to provide rapid controlled environment for micro-sized experiments.

Active microfluidic methods for analysis and manipulation of biological cells have been done in various way and form. In 2003, Wheeler \textit{et al} developed a novel microfluidic device from poly-dimethylsioxane using multilayer soft lithography technology for the analysis of single cells \cite{wheeler2003microfluidic}. The microfluidic setup facilitates the passive and gentle separation of a single cell from the bulk cell suspension. This in turn enables the precise delivery of reagents as little as one nanoliter to the cell. In other use cases, the optical-based microfluidic methods have been used to sort cell with very high accuracy \cite{macdonald2003microfluidic}. This family of method utilizes the fact that different dielectric particles respond differently to an applied light field \cite{tatarkova2003brownian}. Combined with the miniscule spatial setting, the methods are compatible for single-cell resolution analysis. For example, optical-based microfluidic methods have been used to sort cells with very high accuracy \cite{macdonald2003microfluidic, wang2005microfluidic, baret2009fluorescence}.

As a method, passive mocrofluidic methods are mostly used to provide specialized environment in cell-size resoultion. For example, microfluidic settings have been used to keep spatio-temporal identity of single cell for the analysis of the underlying biological dynamics of the isolated cells \cite{mu2013microfluidics, sekhavati2015marker}, which form the methods the design and synthesis of our micro-trench system are based on.

In the last decades, the recent advances in both passive and active microfluidics have created an entire field \cite{whitesides2006origins} with use cases ranging from \textit{in vivo} imaging \cite{chronis2007microfluidics}, single-cell analysis \cite{wheeler2003microfluidic} to cellular biophysics \cite{di2010bacterial}. This project leverages in these advances coupled with equally outstanding advances in image acquisition, image processing and machine learning methods which will be described in the subsequent subsections.

\subsection{Image Processing and Computer Vision}

\label{subsection:cv_advances}

Before the runaway advance of deep learning methods in the last years, object and area detection methods generaly rely on disciminating certain patterns and features in the picturek -- with and without utilizing machine learning methods. In 2001, Paul Viola and Michael Jones proposed a method that was able to recognize face \cite{viola2004robust}, and later, objects in almost real-time fasion \cite{viola2001rapid}. %TODO continue with viola jones

%TODO fill

\subsection{Machine Learning}

\label{subsection:ml_advances}

%TODO fill

\section{Fully automated pipeline for single-cell analysis}

\subsection{Comparison with other methods}

\subsubsection*{Cell death determination}

Measuring cell death is a crucial part of the experiment, as the reliable determination of it is the basis of most analysis in this thesis. There are several way to measure cell deaths with varying complexity and accuracy. Each method contains certain assumptions of cell death.\\

For example, determining cell death by cell movement assumes death of a cell if no movement beyond random flux is observed in certain amount of time. This obviously has certain drawbacks, such as when the observation is done in non-static environment. Moreover, defining the limit of the random flux, above which a given cell is assumed to actively move, is not a trivial task. Some kind of gold standard for a given cell line and environment has to be established manualy, which is very time consuming. This fact is again made even more complicated by the fact that many cells show different movement pattern upon introduction of treatment. It is well known that some cells tend to move faster or slower under stress, the situation many cancerous cells in our experiment will experience upon addition of cancer drugs treatment \cite{pienta1991effects, fenteany2003small, ruocco2012suppressing}.\\

The second method is using cell size. During apopotosis, the cells would shrink. Given It is known that cell size %TODO continue.



\section{The structure of the thesis}

%TODO fill

The thesis is presented as two closely-related things. First, we present high throughput system that enables analysis in single-cell resolution of cells' response towards certain chemical reaction and a software suite that processes, analyzes and visualizes the data. This is an end-to-end solution that possibly can help researcher in their research. Second, we apply this method on the main question of the project: the response dynamics of cancer cell towards chemotherapeutic treatment. Here several questions are posed and addressed using the system and software suit.

%TODO improve

%TODO add chapter one

Chapter \ref{chapter:data_and_method} contains %TODO continue

\chapter{Data and Methods}

\label{chapter:data_and_method}

As mentioned in previous parts of this thesis: this project consists of three part -- the problem statement regarding the dynamics of cancerous single-cells under pressure of treatment, the microfluidic system which enables the single-cell protocol and the software implementation used to process and analyze the time-lapse data coming out of the experiment. 

This chapter considers two aspects of the project: the experimental setting and the theoretical aspects behind the data analysis pipeline. In the first half of the chapter, we deal mostly with the experimental background and the underlying questions of single cell dynamics of cancer cell under stress with focus on chemotherapeutic pressure. In this part, the highlight of the experiment, the microfluidic system for cell containment is elucidated. In conjunction with the system, some biomedical and biochemical aspects of the experiment are also mentioned. This includes the drug, the auxiliary chemicals used in the experiment and the cell lines probed.

The second part deals mostly with the quantitative methods and algorithms used to process data into meaningful observations. This part is opened with definitions used in the methods section. Afterwards, every method developed/used in the pipeline is brought forward with accompanying rationale and literature research.

\section{Experimental setting and data}

\subsection{Cell culture}
\label{subsection:cell_culture}

To enable cross reference and comparability of experiment results, a model cell line is used: the acute monocytic leukimia (AML) cell line \textbf{AML-M5a MOLM-13}. The line used in our experiment was derived from the cell line initially described by Matsuo et al in 1997 \cite{matsuo1997two}. In the paper, the authors developed the line from the peripheral blood of a relapse patient with AML of subtype FAB M5a, which is characterized by predominantly monoblastic leukemia cells visible in pap smear \cite{arber20162016}. Due to extensive research done on the cell line and the well-explained mechanism of the cell line dynamics and response towards cancer medication, the cell line is an ideal candidate for \textit{in vitro} study of monocytic differentiation, leukemogenesis and treatment dynamics \cite{matsuo1997two, kelly2002ct53518, yokota1997internal}.

For our experiment, the AML-M5a MOLM-13 cell line was cultured in Gibco\textsuperscript{\textregistered} RPMI 1640 GlutaMAX medium, produced by Life Technologies \cite{gibcocellculture2017}. The medium is popular choice in human cell biology for both experiments and biological syntheses using human cells and their derivatives \cite{blight2000efficient, shimizu2002fabrication}. It is pre-supplemented with stable form of L-glutamine to prevent ammonia buildup, a common and serious problem in cell culture due to its cell toxicity\cite{satter1974effect}. The medium is further supplemented with Gibco\textsuperscript{\textregistered} Fetal Bovine Serum (FBS), also offered by Life Technologies \cite{gibcofbs2017}, as supplement for the AML-M5a MOLM-13 cell culture.

Some other cell lines were examined as potential probe cell line in this experiment. One of them is Jurkat Cell, a model cell commonly used to study T Cell Leukimia, T cell signalling mechanism and the expression of various HIV-related chemokines \cite{schneider1977characterization}. The cell line was a considered since it is well-studied \cite{johnson2007genome, schena1996parallel}. This is especially true if we consider apoptotic mechanism of the cell line, a problem this project and other related projects by our  and partner labs are trying to investigate. There are several seminal publications about the dynamics of apoptotic mechanism of Jurkat cells we could well compare our results to \cite{gottlieb1996apoptosis}. Samali \textit{et al} \cite{samali1999presence} even studies the dynamics of caspase expression in Jurkat cells, a topic dealt a lot in this project as the chapters progress (see for example Subsection \ref{subsection:treatment}) while Kasai \textit{et al} considers the aspect of spindle checkpoint in the context of apopototic cell death \cite{kasai2002prevalent}. However, we figured out early on that the cell motility of the Jurkat cell line was increased dramatically (a phenomenon observed by others before us \cite{barnhart2004cd95}) upon the introduction of chemotherapeutic treatment -- the increase dramatic enough that the cells managed to escape the micro-trench it initially landed in.


\subsection{Microfluidics}
\label{subsection:microfluid_env}

%TODO change this subsection: see Carsten's comments

In our cases, the microfluidic settings trace back to the works of our partner lab at Biophysics Department at Ludwig-Maximillians-Universit\"at M\"unchen in 2013 \cite{marel2013arraying} and 2015 \cite{sekhavati2015marker, sekhavati2015dynamic}.

In order to track in a label-free manner non-adherent cells over several generations, we designed and fabricated micro-trenches ($30 \times 120 \, \mu m$) out of PEG-DA (Polyethylene(glycol) Diacrylate), which can accommodate four to six cells. The proposed platform facilitated cell tracking leading to the observation of hundreds of families of cells, derived from one single mother at each case. This enabled us firstly to study the distribution of division times among single cells and also to correlate the division times between sister cells, which are genetically identical. Secondly, the array of micro-trenches enabled the study of the response dynamics of single-cells to doxorubicin, a widely used chemotherapeutic drug, and the comparison of the response to this agent between a chemically synchronized and a non-synchronized population. The detailed methods and protocols for the fabrication of the microfluidic system used in the experiment could be found in Appendix \ref{appendix:microtrench}. The design of the micro-trench and the schematic representative of cell tracking are seen in Figure \ref{fig:microtrench_design}.

The experiment setting looks as follows:

\subsubsection*{The micro-trenches:}

The smallest structure of the setting, measuring about 120 microns by 30 microns. The base of the trench is made of Polyethylene (glycol) Diacrylate (PEGDA), an inert substance commonly found as construction material in microfluidic system \cite{sekhavati2015marker}. Each tretment contains about 2400 micro-trenches (See \textbf{Results} section) contained in one containment box.

\subsubsection*{The containment system:}

The trench could contain up to 8 cells. The macro-container chosen for containing the wafers holding the micro-trenches is ibidi\textsuperscript{\textregistered} sticky-Slide 8 Well (see Figure \ref{fig:ibidi}). In the project, each cell treatment is isolated in one containment box. This ensure the separations of different chemicals used in each treatment. 

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/sticky-slide-8-well-marked}
\caption{ibidi\textsuperscript{\textregistered} sticky-Slide 8 Well. The base SU-8 wafer is located in each of the containment box \textbf{(A)}. The SU-8 wafer is then fabricated in the surface of each containment box using nano photolitographic printing. The microfluidic system is then poured and stamped on top the wafer (see Appendix \ref{appendix:microtrench} for detailed manufacturing process). Note that each containment box is upside-open. The cap (\textbf{(B)} is used to prevent the ingress of foreign materials into the medium. \textit{Image taken and modified from ibidi GmbH's website}.}
\label{fig:ibidi}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{images/trenches-sekhavati}
\caption{The structure of micro-trenches: \textbf{(A)} 3D model of micro-trench on surface with cells inside. \textbf{(B)} The schematic representation of a time-lapse in a trench. First, a singly-placed cell is tracked in a micro-trench. At time $t_0$, the cell divides into two daughter cells. The two cells will be kept tracked until at one point each of the daughter cells will divide at the same time at time $t_{div}$. Note the simplification of the sample. First, not every cell is singly-placed inside a trench. Indeed, not every trench is occupied by cells. Second, not every cell divides. Third, not every cell line observed has three generations in it. And finally, not every children's division times are at the same time. Indeed, this special case almost never happens in real life. \textbf{(C)} The sample view into the environment with cells occupying some micro-trenches. Here, the micro-trenches have dimension of 120 $\mu m$ long and 30 $\mu m$ wide. Note also the pointish characteristic of the cells taken in out-of-focus fashion. This improves the performance of the tracking algorithms. \textit{Figure taken from (Sekhavati, 2015) \cite{sekhavati2015dynamic}}.}
\label{fig:microtrench_design}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/microtrench_in}
\caption{The typical view of micro-trench setting. Some trenches contain no cell at all \textbf{(A)}. Several trenches contain exactly one cell \textbf{(B)}. A few more trenches contain two cells \textbf{(C)} while in rare cases the trench may contain more than two cells \textbf{(D)}.}
\label{fig:microtrench_sample}
\end{figure}

\subsection{Cancer treatment regimes}
\label{subsection:treatment}

As mentioned in previous chapters, the objective of the project partains mostly the dynamics of single cancer cell under treatment. After mentioning the cell lines of interest (Chapter \ref{subsection:cell_culture}) and the microfluidic setup used to contain cancer cells in single-cell setting (Chapter \ref{subsection:microfluid_env}), we arrived at the last aspect of the experimental seting: the chemical treatment used on the cells.

For cell cultures mentioned in chapter Subsection \ref{subsection:cell_culture}, two treatment regimes are developed : Vincristine and Daunorubicin -- both chemotherapeutic compounds well known for treating various kinds of cancer \cite{drugs2018defdaunorubicin, ravina2011evolution, tsuruo1981overcoming, gewirtz1999critical}.

\subsubsection*{Vincristine}

Vincristine is initially isolated from Madagascar pairwinkle \textit{Catharanthus roseus} (basionym \textit{Vinca roseus}, hence the name) \cite{nci2018defvincristine}. It is mainly known as Tubulin polymerase inhibitor \cite{tsuruo1982increased}, a subclass of mitotic inhibitor family of drugs \cite{jordan1998tubulin}. Mechanistically, it prevent Tubulin polymerization in two ways. First, it binds elongating Tubulin polymer and reduces the affinity of the elongating polymer \cite{lobert1996interaction} towards the monomers that are supposed to conjoin the polymer thereby extending it. Meanwhile, further elongation by the polymers are also prevented via allosteric inhibition (inhibition caused by spatial occupancy of inhibiting agent) of polymerating Tubulin via Vincristine. The allosteric competitor is hence commonly known as vinca alkaloid binding domain and commonly seen in other tubulin polymerization inhibitor compounds \cite{tsuruo1982increased}. It has also been shown that Vincristine also depolymerizes stable microtubuli in rat axonal part of rats' neuronal cells \cite{jordan1998tubulin}. Thus, the effect of Vincristine is most emphasized during the time of high Tubulin synthesis: during the separation of chromosomes in Metaphase by means of tearing them with the simultaneous pulling and pushing mechanism of Tubulin poly- and depolymerization \cite{owellen1972binding}. In the context of chemotherapy, Vincristine is often as combination in CHOP (cyclophosphamide, doxorubicin, vincristine, and prednisone) regime \cite{hiddemann2005frontline} against non-Hodgkin's lymphoma; MOPP \cite{brandriff1994chromosomal}, COPP \cite{pfreundschuh1987lomustine} and BEACOPP \cite{diehl1998beacopp} regimes against Hodgkin's lymphoma; and Stanford V regimes against acute lymphoblastic leukimia \cite{bartlett1995brief}. It is also used to certain degree as immunosuppresant due to its mitotic inhibiting characteristics \cite{ahn1974vincristine}.

\begin{figure}[H]
\centering
\begin{subfigure}{.8\textwidth}
  \centering
  \includegraphics[width=.6\textwidth]{images/vincristine}
  \caption{}
  \label{fig:vincristine}
\end{subfigure}
\centering
\begin{subfigure}{.9\textwidth}
  \centering
  \includegraphics[width=.8\textwidth]{images/vincristine_mechanism}
  \caption{}
  \label{fig:anthracyclines_dna}
\end{subfigure}
\caption{(a) Molecular structure of Vincristine. (b) Visualization of Vincristine's mechanism of action. During normal metaphase, microtubuli elongate from centrosome towards equator and bind fully replicated chromosomes. The microtubuli then pull chromosome pair apart during anaphase. Vincristine and other vinca alkaloids prevent this from happening by competitive inhibition, allosteric inhibition and active depolymerization of extending microtubuli. Unsuccessful exit metaphase forces the cell to undergo programmed cell death.}
\label{fig:vincs}
\end{figure}

\subsubsection*{Daunorubicin}

Daunorubicin is initially isolated from bacterium \textit{Streptomyces peucetius} \cite{otten1990cloning, lin2011chiral}. It is part of anthracycline class of drug \cite{gewirtz1999critical} extracted mainly from \textit{Streptomyces} bacteria. Some well-known members of this class are Doxorubicin \cite{nci2018defdoxorubicin}, Epirubicin \cite{nci2018defepirubicin} and Idarubicin \cite{nci2018defidarubicin}. Together, they are among the most effective cancer drugs ever deployed and are used towards more kinds of cancer than other group of chemotherapeutic agents \cite{weiss1992anthracyclines, minotti2004anthracyclines, peng2005cardiotoxicology}. Like many chemotherapeutic agents including Vincristine, anthracyclines attack cancerous tissues and cells by preventing their division \cite{drugs2018defdaunorubicin}. Unlike vinca alkaloids however, anthracyclines prevent the division by disrupting another mechanistic part of cell division: the DNA polymerization \cite{gewirtz1999critical}. There are four ways anthracyclines disrupt DNA polymerization:

\begin{itemize}
\item Anthracyclines intercallate with base pairs involved in polymerization thus preventing strands extension \cite{takimoto2008principles}.

\item Anthracyclines covelatently inhibit of type II topoisomerase which is responsible for RNA and DNA supercoil relaxation \cite{wang2002cellular}. Inhibition of type II topoisomerase causes supercoiled RNA and DNA to be inaccessible for initiation of duplication thus breaking the DNA \cite{tewey1984adriamycin}.

\item Anthracyclines induce oxidative stress on cancer cell organelles by generating free oxygen radicals. These radicals in turn damage DNA, proteins and cell membranes and initiate caspase induced apoptosis \cite{vsimuunek2009anthracycline, halliwell1994free}.

\item Anthracyclines disrupt epigenetic, transcriptomic and DNA upstream regulations by removing histones from DNA strands \cite{pang2013drug}. This also exposes DNA strands to degradation factor such as DNA methylase \cite{vaissiere2008epigenetic} and oxidative damage \cite{ljungman1992efficient}

\item In presence of formaldehyde, anthracyclines cross-links with DNA covalently, creating cytotoxin that will disrupt DNA from functioning properly \cite{wang1991formaldehyde}.
\end{itemize}

In normal chemotherapeutic regime, both Vincristine and Daunorubicin are prescribed intravenously to the patients \cite{skeel2011handbook}. Needless to say, both drugs will disrupt both cancerous and healthy cells. The effect is however mostly felt in actively dividing cells and organs such as blood cells  and hair follicles due to chemotherapeutic agents' mostly disruptive effect during cell division as mentioned above \cite{skeel2011handbook, mayo2018chemotherapy}.

\begin{figure}[H]
\centering
\begin{subfigure}{.8\textwidth}
  \centering
  \includegraphics[width=.5\textwidth]{images/daunorubicin}
  \caption{}
  \label{fig:dauno}
\end{subfigure}
\centering
\begin{subfigure}{.8\textwidth}
  \centering
  \includegraphics[width=.5\textwidth]{images/doxorubicin_dna}
  \caption{}
  \label{fig:anthracyclines_dna}
\end{subfigure}
\centering
\begin{subfigure}{.9\textwidth}
  \centering
  \includegraphics[width=.8\textwidth]{images/daunorubicin_mechanism}
  \caption{}
  \label{fig:dauno_mechanism}
\end{subfigure}
\caption{(aonoaunos}
\end{figure}

\subsection{Cell death signal}
\label{subsection:cell_death_signal}

As mentioned in Subsection \ref{subsection:treatment}, the introduction of cancer treatment disrupts mitotic cell activities which in turn activates cell death pathways. Two biochemical signals are selected to observe cell death caused by the disruption of mitotic process: %Caspase3/7 and Propium Iodide (PI).

\subsubsection*{Caspase 3/7}

Caspase 3 and 7 are both known as executioner Caspases in programmed cell death pathways \cite{alberts2017molecular}.  The reagent consists of a four amino acid peptide (DEVD) conjugated to a nucleic acid binding dye. This cell-permeable substrate is intrinsically non-fluorescent, because the DEVD peptide inhibits the ability of the dye to bind to DNA. After activation of caspase-3 or caspase-7 in apoptotic cells, the DEVD peptide is cleaved, enabling the dye to bind to DNA and produce a bright, fluorogenic response.  It has excitation/emission maxima of ~502/530 nm (see Figure \cite{fig:caspase_spectra}). For the project CellEvent Caspase3/7 vial from ThermoFisher Scientific is used  \cite{thermofisher2018casp}.

\begin{figure}[H]

\centering

\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/caspase3_dimer}
  \caption{}
  \label{fig:caspase3_dimer}
\end{subfigure}%
~
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/caspase7_dimer}
  \caption{}
  \label{fig:caspase37_dimer}
\end{subfigure}%

\centering

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\textwidth]{images/caspase37_spectra}
  \caption{}
  \label{fig:caspase_spectra}
\end{subfigure}%

\caption{a) Molecular structure of Caspase 3 heterodimer b) Molecular structure of Caspase 7 homodimer. The p17 (light blue) and p12 (pink) subunites are show. c) The excitation (blue) and emission (red) spectra of Caspase3/7 vial by ThermoFisher Scientific. Notice the peaking around the 525\~535 nm range in green channel. Image taken from ThermoFisher Scientific product specification page \cite{thermofisher2018casp}.}

\label{fig:caspase_stain}
\end{figure}

\subsubsection*{Propium Iodide (PI)}

PI is membrane impermeable and generally excluded from viable cells. Therefore it is commonly used for identifying dead cells in a population and as a counterstain in multicolor fluorescent techniques. PI binds to DNA by intercalating between the bases with little or no sequence preference and with a stoichiometry of one dye per 4-5 base pairs of DNA \cite{suzuki1997dna}.  In aqueous solution, the dye has excitation/emission maxima of 493/636 nm (see Figure \ref{fig:pi_spectra}). PI also binds to RNA, necessitating treatment with nucleases to distinguish between RNA and DNA staining. Once the dye is bound to nucleic acids, its fluorescence is enhanced 20- to 30-fold. For the project PI vial from ThermoFisher Scientific is used \cite{thermofisher2018pi}.

\begin{figure}[H]
\centering

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.4\textwidth]{images/pi}
  \caption{}
  \label{fig:pi_molecule}
\end{subfigure}%

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\textwidth]{images/pi_spectra}
  \caption{}
  \label{fig:pi_spectra}
\end{subfigure}%

\caption{a) Molecular structure of Propium Iodide (PI) b) The excitation (blue) and emission (red) spectra of PI. Notice the peaking around the 600\~600 nm range in red channel. Image taken from ThermoFisher Scientific product specification page \cite{thermofisher2018pi}.}

\label{fig:pi_stain}
\end{figure}

\subsection{Cell treatments}
\label{subsection:cell_treatments}

Generally, following questions are to be investigated in the experiment:

\begin{itemize}
\item We would like to investigate the dynamics of the AML-M5a MOLM-13 cell line's response towards chemotherapeutic treatment regimes in various drugs' concentration levels (for both Vincristine and Daunorubicin).
\item We also would like to look into the effect of cell cycle on the the AML-M5a MOLM-13 cell line's response to drugs. Specifically, following questions needs to be answered: does the response of cancer cells depends on cell cycle?
\end{itemize}

To answer aforementioned questions, we designed the experiments as follow. First, two experiments are done:

\subsubsection*{Unsynchronized experiment.}

In the unsynchronized experiment, all 8 wells of the ibidi\textsuperscript{\textregistered} sticky-Slide 8 Well ('slide') are used with one microtrench wafer in each well (see Subsection \ref{subsection:microfluid_env} for the description of the slide). Wvery well is poured with different concentrations of treatments. To cover as many concentration range as possible, logarithmic scale is used to select concentration starting with 1 nanomolar (nM) all the way to 1000 nM. The use of logarithmic scale is also in line with logarithmic nature of many enzymatic and molecular process in cell biology \cite{wilkinson1961statistical, savageau1969biochemical}. Following concentrations are used:  1 nM, 5 nM, 10 nM, 100 nM and 1000 nM of Vincristine; 10 nM and 100 nM of Daunorubicin; and control concentration (0 nM). The unsynchronized experiment concentration configuration can be seen in Table \ref{table:unsyn_treatments}. The recording lasts for 45 hours. After 21 hours of recording, the chemotherapeutic treatment is introduced into each well. Table \ref{table:image_capture_frequency} shows the image capturing frequency for each image type.

\begin{table}[H]
\centering
\begin{tabular}{| l | c |}
\hline
Condition & Positions \\
\hline
A1: 1000 nM - VCR & 1-8 \\
A2: 100 nM - VCR & 9-15 \\
A3: 10 nM - VCR & 16-23 \\
A4: 1 nM - VCR & 24-31 \\
B4: 10 nM - Dauno 32 & 32-39 \\
B3: 100 nM - Dauno & 40-47 \\
B2: 0 nM - VCR & 48-55 \\
B1: 5 nM - VCR & 56-63 \\
\hline
\end{tabular}
\caption{Configuration table of treatments for synchronized experiment}
\label{table:unsyn_treatments}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{| l | l | c |}
\hline
What & What for & $t$ \\
\hline
In-focus brightfield (BF) image & Sanity check, micro-trench masking & 30 minutes \\
Out-of-focus BF image & Image tracking, image analysis & 15 minutes \\
Red fluorescence channel (PI)& Image tracking, image analysis & 15 minutes \\
Green fluorescence channel (Csp3/7) & Image tracking, image analysis & 15 minutes \\
\hline
\end{tabular}
\caption{Image capture frequency of the unsynchronized experiment}
\label{table:image_capture_frequency}
\end{table}


\subsubsection*{Synchronized experiment}

The main difference between unsynchronized and synchronized experiments is, as the name already suggests, the synchronization step done in synchronized experiment. The process involves arresting cell cycle with the so-called "double thymidine block" protocol \cite{harper2005synchronization}, which arrests cell development at $G_1/S$-phase by stopping DNA synthesis using double thymidine block, a well known DNA synthesis inhibitor \cite{bostock1971evaluation}.

Like unsynchronized experiment, several concentration levels are tested in this experiment (see Table \ref{table:syn_treatments}). The image capturing starts after the cells are synchronized and lasts 24 hours. The image capturing frequency is exactly the same as unsynchronized experiment's (see Table \ref{table:image_capture_frequency}).

\begin{table}[H]
\centering
\begin{tabular}{| l | c |}
\hline
Condition & Positions \\
\hline
A1: 1000 nM - VCR & 1-8 \\
A2: 100 nM - VCR & 9-15 \\
A3: 10 nM - VCR & 16-23 \\
A4: 1 nM - VCR & 24-31 \\
B4: 0 nM & 32-39 \\
B3: 5 nM - VCR & 40-47 \\
B2: 10 nM - Dauno & 48-55 \\
B1: 100 nM - Dauno & 56-63 \\
\hline
\end{tabular}
\caption{Configuration table of treatments for synchronized experiment}
\label{table:syn_treatments}
\end{table}

The detailed image capturing protocol for both unsynchronized and synchronized experiments can be seen in Appendix \ref{appendix:imageacquisition}.


\section{Definitions}

This section contains formal definitions and methods used in this thesis.

\subsection{Image encoding}
\label{subsection:image_encoding}

From the initial recordings on, there are several image standards and encodings being processed and analyzed in our pipeline. We would start from the raw image captured by recording apparatus going down to processed images (See Appendix \ref{appendix:imageacquisition} for detailed protocols on image acquisition).

There are two kind of recordings taken in our project: brightfield channel and fluorescence channel recordings. Two brightfield channels are used in this experiment:

\begin{itemize}
\item In-focus brightfield image. The focal distance of the camera $d_f$ is exactly the same as the distance from the apparatus to the microfluidic system $d_m$, i.e. $d_f = d_m$.
\item Out-of-focus brightfield image. To capture out-of-focus brightfield image, $d_f$ should be different than $d_m$. Several distances were tried during the experiments (see Figure \ref{fig:focustest}). Difference in focus-microfludics distance influences the quality of the image in several ways (see Figure \ref{fig:focustest}). It was determined that the best resulting image came from the lense with $d_f - d_m = -20 mu m$.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/focus/pict.png}
\caption{Comparison between in-focus image and out-of-focus images taken at various focus-microfluidics difference $d_f - d_m$. The images were taken from the section in the well. In the first row are images with focus behind the microfluidics system while in the second row are images with focus before the system. A: in focus image. B: $+10 \mu m$. C: $+20 \mu m$. D: $+30 \mu m$. E: $+40 \mu m$. F: $+50 \mu m$. G: $-10 \mu m$. H: $-20 \mu m$. I: $-30 \mu m$. J: $-40 \mu m$. K: $-50 \mu m$.}
\label{fig:focustest}
\end{figure}

To detect cell death signal, fluorescent channels are used: red and green. These channels only capture fluorescence emission in corresponding spectrum area (i.e. 620–750 $nm$ and 495–570 $nm$ respectively):

\begin{itemize}
\item Red fluoroscent channel. This channel is used to capture emission coming out of PI activation due to PI's DNA binding emission being mostly in red wavelength (see Subsection \ref{subsection:cell_treatments}).
\item Green fluoroscent channel. This channel is used to capture emission coming out of caspase3/7 activation due to caspase3/7's binding emission being mostly in green wavelength (see Subsection \ref{subsection:cell_treatments}).
\end{itemize}

From the recording apparatus, the image came as TIFF image (see Appendix \ref{appendix:imageacquisition} for the detailed protocols) \cite{loc2006tiff}. Each recorded stack consists of slices which. One slice represent a single image capture at a time point. Each stack is encoded using RGB color model. This includes fluorescent image in which each slice shows the intensity in corresponding color channel (red, green or blue) as monochromatic RGB image. 

RGB color model represents the pixel as a combination of red, green and blue color. This encoding is able to represent various spectra of human visible color and useful enough for most use cases \cite{sonka2014image, jayant1993signal}. The most commonly used RGB encoding is the 8 bit encoding. Here, each pixel is represented as an RGB pixel having red, green and blue color values ranging from 0 to 255. Mathematically, this means that each pixel $M_{t,x,y}$ of a slice captured at time $t$ in this image can be represented a triple, i.e.:

\begin{equation}
\label{equation:rgb_pixel_def}
M_{t, x, y} := (M_{t, x, y, 1}, M_{t, x, y, 2}, M_{t, x, y, 3})
\end{equation}

with

$$
M_{t, x, y, 1} M_{t, x, y, 2} M_{t, x, y, 3} \in [0,255]
$$

for 8-bit RGB encoding. Consequently, a slice $M_t$ of width $w$ and height $h$ is a 3-dimensional matrix of dimension $h \times w \times h$, i.e.

$$
M_i \in p_c^{w \times h \times 3}
$$ 

with $p_c \in [0:255]$. A stack (a sequence of slices) $M$ of $n$ slices is in turn a 4-dimensional matrix:

$$
M_i \in p_c^{n \times w \times h \times 3}
$$ 

In some steps in the analysis pipeline, the image will modified to other encoding and vice versa, e.g. from 8 bit to 16 and from 8 bit to 32 bit RGB encoding. To convert RGB value of a pixel $M^a_{x,y}$ from one encoding to another $M^n_{x,y}$, linear conversion is normally used:

\begin{equation}
\label{euqation:linear_conv_rgb}
M^b_{x,y} = (\lceil M^a_{x,y, 1} \frac{2^b}{2^a} \rceil, \lceil M^a_{x,y, 2} \frac{2^b}{2^a} \rceil, \lceil M^a_{x,y, 3} \frac{2^b}{2^a} \rceil)
\end{equation}

where $a$ and $b$ refer to the bit length of the source and target encoding respectively (commonly known as \textbf{bid depth}).  Commonly used depths are 8, 16 (\textit{high color} format), 24 (\textit{true color} format) and 48 bits (\textit{deep color} format) \cite{lim1990two, sharma1997digital, sullivan2012overview}. 

Sometimes, both grayscale and 1-bit monochrome encoding is used/produced during image analysis. A grayscale image essentially shows the intensity of an image. A grayscale slice $G$ can be represented as matrix of integer, i.e. $G \in p_c^{w \times h}$ for 8-bit grayscale for example. Like RGB image, linear scaling can be applied to transform grayscale images across bit depth:

\begin{equation}
\label{equation:linear_conv_gray}
G^b_{x,y} = \lceil G^a_{x,y} \frac{2^b}{2^a} \rceil
\end{equation}

RGB image can be transformed to grayscale image by combining the intensity from every channel:

$$
G_{x, y} = \frac{M_{x, y, 1} + M_{x, y, 2} + M_{x, y, 3}}{3}
$$

this however does not reflect human perception of light, as human eyes' spectral sensitivity is not uniform across sensitivity spectrum \cite{wyszecki1982color}. Indeed, as Osorio and Vorobyev shows in 2005, each species has its own specific spectral sensitivity distribution \cite{osoosorio2005photoreceptor}, as can be seen in following figure:

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/spectral_sensitivity}
\label{fig:spectral_sensitivity}
\caption{Spectral sensitivity of human (a), honeybee (b), pigeon (c) and home fly (d). Note that the sensitivity distribution is maximum normalized (i.e. each value in a curve is relative only to the maximum value of the curve). Each species has its own set of cone cells, as can be seen in the variation of the number of curves for each species. D refers to double cone, while SVF refers to short visual fiber and LVF to long visual fiber. The y's and p's indicate the yellow/pale sensitivity of house fly's long visual fiber. Figure taken from (Osorio and Vorbyev, 2005 \cite{osorio2005photoreceptor}).}
\end{figure}

There are some conversions published on transforming RGB value to human intensity perception based on the measured spectral sensitivity \cite{anderson1996proposal, itu2007studio, itu2015parameter}. Among the most commonly used is the \textit{BT.601} standard from International Telecommunication Union (ITU). It recommends following luminosity weight for RGB to grayscale conversion \cite{itu2007studio}:

\begin{equation}
\label{equation:rgb_to_gray_def}
G_{x, y} = \frac{0.299 M_{x, y, 1} + 0.587  M_{x, y, 2} + 0.114 M_{x, y, 3}}{3}
\end{equation}

Reverting back grayscale to RGB image in turn only consists of applying grascale value to each color component:

$$
M_{x, y} := [G_{x, y}, G_{x, y}, G_{x, y}]
$$

A 1-bit monochrome image is encoded as binary matrix:

$$
B \in p_b^{w \times h} \land p_b \in {0, 1}
$$

this encoding is superior to other encoding for uses cases that do not require complete information of the image but rather separation of interesting parts in the image, since since it requires less memory (1 bit per pixel vs 24 bits per pixel of normal RGB image) and enables bitwise operation native to CPU \cite{kernighan1988c}. 

Some use cases for this encoding are for example region of interest (ROI) bounding and contour and boundary visualization \cite{hartley2003multiple}.

The conversion from RGB to binary image can be done by defining cutoff value $c$, i.e.

\begin{gather*}
B_{x, y} =
\begin{cases}
  1 & \text{if } M_{x, y} > c\\    
  0 & \text{else}  
\end{cases}
\end{gather*}

Note that, unless otherwise mentioned, every bit encoding of an image refers to unsigned encoding. Thus, an $m$-bit encoding allows value ranging from $0$ to $2^m - 1$.

\subsection{Brightness and contrast adjustment}

We consider following image: 

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/pos_41_in_t0}
\caption{The in-focus image of position 41 from unsynchronized experiment.}
\label{fig:pos41}
\end{figure}

using RGB to intensity conversion formula shown in Subsection \ref{subsection:image_encoding}, we can convert the image into grayscale image representing the intensity. The grayscale image has the intensity distribution as shown in following histogram:

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/pos_41_in_t0_hist}
\caption{Normalized distribution of intensity of Figure \ref{fig:pos41}.}
\label{fig:pos41_density}
\end{figure}

Mathematically, we can represent brightness and contrast adjustment as a mapping between one domain to another one. Let $l(M_{t, i, j})$ a function that maps the RGB value of a pixel $M_{t, i, j}$ to its corresponding intensity. Conventiently, we can use ITU's \textit{BT.601} standard the formula written in Subsection \ref{subsection:image_encoding}.

We then define the $I: [0:w] \times [0:h] \rightarrow \mathbb{N}_0$ function that maps the coordinate of a pixel to its corresponding intensity value i.e.

$$
I_t(i, j) := l(M_{t, i, j})
$$

Note that the codomain of the function $I_t$ depends on the encoding used. For a 8-bit encoding this will be then $[0:255]$.

An intensity transformation $J_t$ is then defined as a linear transformation of $I_t$ with \textbf{gain} and \textbf{bias} parameters $\alpha$ and $\beta$ respectively,

\begin{equation}
\label{equation:br_ct_transform}
J_t(i, j) := \alpha \cdot I_t(i, j) + \beta
\end{equation}

with $\alpha > 0$ \cite{szeliski2010computer}. The gain and bias parameters are also known as \textbf{contrast} and \textbf{brightness} parameters accordingly. In this regard, increasing/decreasing brightness is equivalent to increasing/decreasing $\beta$. The same thing also applies for contrast parameter, increasing $\beta$ will increase the brightness of the image.

Predictably, doing transformation over by $J_t$ will inevitably cause the resulting intensity to be outside of the allowed value range $[0:2^m]$ for m-bit encoding. To understand this situation, we first have to consider the boundedness of eyes perception. The argumentation for boundedness can be shown by realizing that the excitation of a neuron follows sigmoid function \cite{gazzaniga2004cognitive}. Thus given no impuls the neuron will stay in ground state, while very large impulse is bounded due to biochemical constraint of a neuron. Mapped in the context of sigmoid function, a steady state corresponds to 0 while asymptotically unlimited excitation correspondes to 1. In our context, a non-excited state corresponds to 0 intensity while full-excitation corresponds to $2^m - 1$ intensity. Thus, the equation \ref{equation:br_ct_transform} could be closely bounded by introducing upper and lower bound of $0$ and $2^m - 1$, i.e.:

\begin{gather*}
J_t(i, j) :=
\begin{cases}
  2^m - 1 & \text{if } \alpha \cdot I_t(i, j) + \beta > 2^m - 1\\
  0 & \text{if } \alpha \cdot I_t(i, j) + \beta < 0\\
  \alpha \cdot I_t(i, j) + \beta & \text{else}
\end{cases}
\end{gather*}

We can for example, transform Figure \ref{fig:pos41} using $J_t(i, j)$ with $\beta=100$. $\alpha$ is not changed in this case. Following images show the transformed image and its corresponding normalized intensity distribution:

\begin{figure}[H]
\centering

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.4\textwidth]{images/pos_41_in_t0_br_up100}
  \caption{1a}
  \label{fig:pos41_brup100_bf}
\end{subfigure}%

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.4\textwidth]{images/pos_41_in_t0_br_up100_hist}
  \caption{}
  \label{fig:pos41_brup100_hist}
\end{subfigure}%

\caption{(a) The in-focus image of position 41 from unsynchronized experiment tranoformed with $\alpha=1$ and $\beta = 100$. (b) Normalized distribution of intensity of the transformed figure. Compared to Figure \ref{fig:pos41_density}. Note that the difference in height across bins \textit{vis a vis} Figure \ref{fig:pos41_density} is mainly caused by the definition of bins in the histogram function.}

\label{fig:pos41_brup100}
\end{figure}

Geometrically, brightness adjustment shifts the entire intensity distribution to the right by $\beta$ \footnote{Note the wording. For negative $\beta$, the shift is negative to the right, i.e. to the left}.

We can also try to change $\beta$. This will scale the image's intensity and emphasize image contrast. See for example following image:

\begin{figure}[H]
\centering

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.4\textwidth]{images/pos_41_in_t0_br_times3}
  \caption{}
  \label{fig:pos41_times3_bf}
\end{subfigure}%

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.4\textwidth]{images/pos_41_in_t0_br_times3_hist}
  \caption{}
  \label{fig:pos41_times3_hist}
\end{subfigure}%

\label{fig:pos41_times3}
\caption{(a) The in-focus image of position 41 from unsynchronized experiment tranoformed with $\alpha=3$ and $\beta = 0$. Notice the contrast has increased significantly, especially around area with high intensity differential such us the margin of a micro-trench and the dark spot in the lower left part of the slice. (b) Normalized distribution of intensity of the transformed figure}
\end{figure}

Geometrically, contrast can be understood as the distance of closely resembling pixel \cite{hartley2003multiple}. Two pixels which are very similar will look very different given a high contrast. Brightness, on the other hands, describes the level of visibleness of a pixel.

\subsection{Micro-trench masking}
\label{subsection:micro_trench_masking}

First, note that the following method is only semi- automatically done. The first step (brightness and contrast correction) is done manually in \textit{Fiji} while the Robust Automatic Threshold Selection (RATS) and the holes filling are done automatically, also in \textit{Fiji}.

Several techniques could be applied to highlight certain area in the image. In Subsection \ref{subsection:cv_advances}, several advances in computer vision methods are described. While more general advances in the field of machine learning is chronicled in Subsection \ref{subsection:ml_advances}. While the collection of advanced methods for detection abound, some simple interpretable methods could be used best to detect and mask the micro-trench. In particular,  we can see for example that the area around a micro-trench exposes the so-called strong intensity gradient (see Figure \ref{fig:pos41_brightness}): the area around the marging of a micro-trench is much darker than the other parts of the well. This can be explained by the fact that the light is reflected less around the wall area and thus the intensity decreases. Moreover, the light beam coming out of the camera is not perfectly perpendicular to the well and thus the non-perpendicular reflection is not reflected back to the camera sensor.

Before us, there are several methods that exploit this kind of phenomenon. Cheng et al for example shows it is possible to recognized salient objects in image by using contrast and brightness adjustment \cite{cheng2015global}. Our method on the other hand, goes further by doing robustness improvement by doing noise cancellation step (See \ref{subsubsection:rats}).

\begin{figure}[H]
\centering

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/pos_41_cut_highlight_y_500}
  \caption{}
  \label{fig:pos41_highlight}
\end{subfigure}%

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/pos_41_cut_highlight_y_500_contrast}
  \caption{}
  \label{fig:pos41_contrast}
\end{subfigure}%
\caption{(a) The highlighted area around the 500-th column of the first slice of position 41. The range starts from the 450-th until the 550-th column. (b) The intensity of the red-marked area in (a), the region around micro-trench margin is indicated by the sudden drop of intensity. Note also the slow drift of the intensity as the pixel goes south (right side of the plot). This phenomenon is called \textit{intensity gradient} (see Subsection \ref{subsection:gaussian_blur} for more.}
\label{fig:pos41_brightness}
\end{figure}

We could thus exploit this knowledge by designing micro-trench masking algorithm as follows:

\subsubsection*{Brightness and contrast adjustment of in-focus image}

First, we adjust the brightness and contrast so that the area far away from the margin is encoded as maximum intensity, while the area around the margin is encoded as minimum intensity. One simple and robust way to do this is to reduce the intensity of each position with the maximum intensity value of the region around the margin.

Mathematically, we can define two sets of points, $\mathfrak{M}$ and $\mathfrak{S}$. $\mathfrak{M}$ refers to the set of points around the trench margin while $\mathfrak{S}$ denotes the set of points far away from it. This brings us to the following transformation,

$$
J_i(i, j) := I_t(i, j) + argmax_{(x, y) \, \in \, \mathfrak{M}}{I_t(x, y)}
$$

creating transformed intensity $I^t$.

As can be seen in Figures \ref{fig:pos41_density} and \ref {fig:pos41_brightness}, while sets $\mathfrak{M}$ and $\mathfrak{S}$ are locally separable, across the board this does not seem so clear. We can thus improve the transformation by also adjusting the contrast parameter $\alpha$ by increasing it so that $\alpha > 1$. This brings the pixels with similar intensity values around the decision boundary (somewhere between the two distributions in Figure \ref{fig:pos41_density}) apart and thus ameliorates the determination of decision boundary by the user. Applying this, we have now the new transformation:

$$
J_i(i, j) := \alpha I_t(i, j) + argmax_{(x, y) \, \in \, \mathfrak{M}}{I_t(x, y)} \text{ with } \alpha > 1
$$

creating transformed intensity $I^t$.

Note that this is still not a perfect transformation, as there are some pixels in $\mathfrak{S}$ with intensity lower than the minimum intensity of pixels in $\mathfrak{M}$. Besides doing this manually, we also improve this by refining the transformed intensity $I^t$ further with the next step: Robust Automatic Threshold Selection.

Subfigures \ref{fig:orig_pic} and \ref{fig:brcon_adj} show images of micro-trenches before and after brightness and contrast adjustment. As mentioned, some parts inside the micro-trench still have pixels  that were not transformed to complete white color (maximum intensity). This can however be removed by global and local noise correction through Robust Automatic Threshold Selection (RATS, see next).

\subsubsection*{Robust automatic threshold selection (RATS)}
\label{subsubsection:rats}

Robust Automated Threshold Selection (RATS), based on description by (Wilkinson and Schut, 1998 \cite{wilkinson1998digital}), computes a threshold map for an image based on two criteria:

\begin{itemize}
\item pixel values, and
\item pixel gradients. \cite{fiji2017rats}
\end{itemize}

The gradient is computed using the so-called Sobel kernels\cite{sobel1990isotropic}, a commonly used in computer vision for edge detection as it particularly emphasizes edges in image upon transformation. It is a discrete differential operator which computes an approximation of gradient of the intensity function $J$.

Originally, the Sobel gradient $\mathbb{G}_{x, y}$ of coordinate $(x, y)$ is defined as,

$$
\mathbb{G}_{x, y} := \sqrt{\Delta_x^2 + \Delta_y^2}
$$

Whereas $\Delta_x$ and $\Delta_y$ are defined as,

\[
\Delta_x = 
\begin{bmatrix}
+1 & 0 & -1 \\
+2 & 0 & -2 \\
+1 & 0 & -1 \\
\end{bmatrix}
*
\mathbf{M_t}
\]

and,

\[
\Delta_y = 
\begin{bmatrix}
+1 & +2 & +1 \\
+0 & 0 & 0 \\
-1 & -2 & -1 \\
\end{bmatrix}
*
\mathbf{M_t}
\]

with asterisk symbol $\mathbf{M_t}$ denoting the image at time $t$ and $*$ two dimensional signal processing convoltion \cite{smith1997scientist}. For example, for a $3 \times 3$ matrix,

\[
M_{3 \times 3} = 
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{bmatrix}
\]

and $\Delta_x$, the value of the middle pixel $(2, 2)$ after evaluating the convolution $\Delta_x * M_{3 \times 3}$ is,

\[
\left( 
\begin{bmatrix}
+1 & 0 & -1 \\
+2 & 0 & -2 \\
+1 & 0 & -1 \\
\end{bmatrix}
*
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix}
\right) [2,2]
= 1 \cdot 1 + 0 \cdot 2 + 1 \cdot 3 + -2 \cdot 4 + 0 \cdot 5 + 2 \cdot 6 + -1 \cdot 7 + 0 \cdot 8 + 1 \cdot 9
\]

It has been shown, however, that only taking the quadratic gradient yields comparable result \cite{wilkinson1998digital}. The Sobel operator is thus only defined as quadratic gradient in Fiji RATS plugin,

$$
\mathbb{g_{x, y}} = \mathbb{G_{x, y}^2} = \Delta_x^2 + \Delta_y^2
$$

Pixels having gradients smaller than threshold $\lambda \sigma$ are then removed, where $\sigma$ is noise (generally, standard deviation of the expected background is used, e.g. standrd deviation of the whole image) and $\lambda$ is scaling factor (Wilkinson determined $\lambda = 3$ to be a good approximation. Visually, this step can be understood as removal of background noise from the image.

To refine the filtering further, iterative correction is applied. First, a quadtree construction is applied in an image. Figure \ref{fig:quadtree} gives an example on how a quadree of an image looks like:

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/quadtree}
\caption{Example of quadtree. The left side is the quadtree representation of original image on the right. \textit{Creative Commons courtesy of anonymous.}}
\label{fig:quadtree}
\end{figure}

Within smallest square,a local threshold $T_R$ is computed in following way:

\begin{equation}
T_R = \frac{\sum g_{x,y} \cdot M_{t, x, y}}{\sum g_{x, y}}
\label{eq:local_threshold_rats}
\end{equation}

If a square only contains noises for example, in which case the $T_r$ filters no point at all as it is too low that no points are removed, the local threshold of the parent square is taken instead (note that the quadree is defined as a tree, see for example the original descrption of quadtree by Finkel and Bentley, 1974 \cite{finkel1974quad}). This step is iterated until a parent square with at least one filtered pixel is reached. Visually, the refining step is equal to filtering out area of an image in which the pixels are mostly noise. In the initial global filtering process, many noisy pixels might not be filtered since happen to have gradients larger than $\lambda \sigma$. Using bottom-up these pixels will be eventually filtered out.

Subfigures \ref{fig:brcon_adj} and \ref{fig:rats} show the input and output of  RATS respectively. The noise partaining in a micro-trench is removed upon application of gradient-based filtering. We can that,

\begin{itemize}
\item the first part of RATS ($\mathbb{B}$-filtering by $\lambda \sigma$) is designed to move global level noise  which in this case means white noises coming from non-perfect reflection of light from the well.
\item the second part of RATS (recursive $\mathbb{B}$-filtering by $T_r$)is designed to remove local noise cause by local distortion due to location-specific artifacts (direction of incoming light not perfectly perpendicular with regard to micro-trench's base etc).
\end{itemize}

\subsubsection*{Holes filling}

After brightness and contrast adjustment and RATS, the image now mostly contains the margin of the micro-trenches. We can then fill these holes to create micro-trench masks. The step consists of doing two processes:

\begin{enumerate}
\item Recognition of the outer part of micro-trench contours, followed by
\item Filling of such contour with a new neutral value (black).
\end{enumerate}

Following image explains the definition of contours and their hierarchy:

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/contours}
\caption{Example of contours and their hierarchy. \textbf{Contours 0}, \textbf{1} and \textbf{2} are \textbf{outernmost contours} and of \textbf{hierarchy-0}. \textbf{Contour 2a} is \textbf{child contour} and of \textbf{hierarchy-1}. \textbf{Contour 3} is child contour of \textbf{contour 2a} and of \textbf{hierarcy-2}. \textbf{Contour 3a} is child contour of  \textbf{contour 3} and of \textbf{hierarchy-3}. Both \textbf{contours 4} and \textbf{5} are children of \textbf{contour 3a} and of \textbf{hierarchy-4}. All contours with same hierarchy are topologically equal. Contour Adopted from OpenCV tutorials by Bradski and Kaehler \cite{bradski2008learning}.}
\label{fig:contours}
\end{figure}

There are several well known algorithms for hierarchical contours recognition. In our case, the algorithm of Suzuki and Abe is used \cite{suzuki1985topological}. The algorithm works by following border between two regions of different color pixel by pixel. The border following process is done hierarchically creating tree-like dependency structure of contours. We then take the outermost contour (in the tree context, the uppermost contours) which are then filled. The filling in turn is simply done by assigning neutral value in every pixel located inside each contour, including all pixels inside all of each contour's children contours.

Subfigures \ref{fig:rats} and \ref{fig:fill_holes} show the input and output of the algorithm respectively.


\begin{figure}[H]
\centering

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.28\textwidth]{images/trench_detect_1}
  \caption{}
  \label{fig:orig_pic}
\end{subfigure}%

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.28\textwidth]{images/trench_detect_2}
  \caption{}
  \label{fig:brcon_adj}
\end{subfigure}%

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.28\textwidth]{images/trench_detect_3}
  \caption{}
  \label{fig:rats}
\end{subfigure}%

\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.28\textwidth]{images/trench_detect_4}
  \caption{}
  \label{fig:fill_holes}
\end{subfigure}%

\caption{The upper left corner of position 41's in-focus-image recording at time $t=0$: (a) The original in-focus image of the upper left corner of position 41. (b) The view of the same area as (a) after applying brightness and contrast adustment. (c) The view of the same area after applying RATS on image in (b) showing the contour of micro-trenches in the area. (d) The contours filled with neutral color (black). Note that some micro-trenches are not filled as they are open and thus topologically not the outernmost contours.}

\label{fig:trench_masking}
\end{figure}

\subsection{Gaussian blur}
\label{subsection:gaussian_blur}

Gaussian blurring in the context of image processing is  the process of applying Gaussian noise on an image. The Gaussian function $G: \mathbb{R} \rightarrow \mathbb{R}$ is a function defined as,

$$
G(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{x^2}{2 \sigma^2}}
$$

In 2 dimensional setting, we extend the function to $G: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ defined as,

\begin{equation}
G(x,y) = \frac{1}{{2\pi \sigma^2}} e^{-\frac{x^2 + y^2}{2 \sigma^2}}
\label{equation:gauss_2d}
\end{equation}

Applying this function on a 2D image will create circles centered around the center of the distribution with values around the center distributed normally. This means that, for an arbitrary $\sigma$  and a uniform image with intensity of $n$ in every pixel,  convolving the image with Equation \ref{equation:gauss_2d} centered at point $i, j$ yields a transformed image with the transformed intensity $I^G$ of each point distributed normally around $(i, j)$ with scaling of $n$ and standard deviation $\sigma$,

\begin{equation}
I^G(x, y) = n \cdot \frac{1}{{2\pi \sigma^2}} e^{-\frac{(x - i)^2 + (y - j)^2}{2 \sigma^2}}
\label{equation:gauss_intens_trans}
\end{equation}

The idea of transforming an image with selected center point is however not very practical as the pixels far away from image will have intensity of almost zero. We can however, use this fact (namely, that the intensity of pixels around the center point fall exponentially the further the pixels are away) to introduce dependency of each pixel on its surrounding environment. This will introduce a blur due to pull-down effect of the neighboring pixels. Hence the name \textbf{Gaussian blurring}.

Mathematically, we can express Gaussian blurring process as convolving the pixels around a position $(x, y)$ and take the average of the convolving image as the new intensity value in $(x, y)$. Using the continuous Gaussian intensity transformation defined in Equation \ref{equation:gauss_intens_trans}, the Gaussian convolution  function is defined as follows,

$$
I^{conv_G}(x, y) = \frac{\sum_{x - \lfloor d/2 \rfloor < i < x + \lfloor d/2 \rfloor} \sum_{x - \lfloor d/2 \rfloor < j < x + \lfloor d/2 \rfloor} I^G(i, j)}{(\lfloor d/2 \rfloor)^2}
$$

with $d$ denoting the $L_1$ distance of the blur.  We can furthermore refine the convolution further by using $L_2$ distance (i.e. radius) instead,

\begin{equation}
I^{conv_G}(x, y) = \frac{\sum_{(i, j) \in C}  I^G(i, j)}{w \cdot d}
\label{equation:conv_gaussian_cont}
\end{equation}

for $C := \{(i, j)  \text{  where } d_{L_2}((x, y), (i, j)) \leq d\}$. It can be seen from the equation that increasing the radius $d$ will increase the pull-down effect on the pixel and hence increase the smoothness. We exploit this property to create regional gradient which will be used to normalize background image. By dividing the value of each pixel by the convoluted value of it we can correct the image by its global background noise. The Gaussian blur-corrected intensity $I^{c_G}$ is thus defined as,

\begin{equation}
I^{c_G}(x, y) = \frac{I(x, y)}{I^{c_G}(x, y)} 
\label{equation:gauss_blur_correction}
\end{equation}

Visually, the method can be understood as low pass filter. It removes the higher frequency 2D signals, i.e. the value with high local variance, resulting with an image with lower fidelity and local variance.

\subsection{Contrast limited adaptive histogram equalization (CLAHE)}
\label{subsection:clahe}

CLAHE, initially developed by Karel Zuiderveld in 1994 \cite{zuiderveld1994contras}, is an instance of the class of algorithms called adaptive histogram equalization (AHE) used to improve contrast in an image. An AHE algorithm generally works by transforming each pixel with a transformation function derived from the neighboring region instead of the whole image.  By doing this, an AHE algorithm can count for variation of  brightness and contrasts in areas of the image \cite{pizer1987adaptive}.

The distinction of CLAHE among other AHE algorithms is its contrast limiting property of algorithm. Given a contrast distribution of an area around a given point,  CLAHE conduct following steps:

\begin{enumerate}
\item First, the algorithm takes a predefined clipping value of intensity histogram of the area around a center point.
\item The algorithm then calculates the intensity histogram of the area.
\item For some intensity values there will be more pixels than allowed having the corresponding intensity values.
\item Adapt this intensity distribution by reassigning the intensity of some pixels in the clipping intensity area uniformly across the range of intensity in the image (see upper part of Figure \ref{fig:clahe}).
\item Repeat until for randomly selected areas in the image the clipping constraint is satisfied.
\end{enumerate}

Figure \ref{fig:clahe} visualizes the step 2, 3 and 4 of the algorithm description above. As can be seen in the area intensity CDF in the lower part of the figure, the algorithm results in more equalized intensity values. Visually, this reduces contrast and in turn sharpens the image. Figure \ref{fig:clahe_before_after} shows an area of the well before and after the application of CLAHE on the taken image. Note the increased brightness and emphasized  convolution effect around the micro-trench margin area.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/clahe}
\caption{Desription of CLAHE. Taken from Pizer, Zuiderfeld et al, 1987 \cite{pizer1987adaptive}  Note the  smoothen CDF function (lower part of the figure) after reassigning some pixels with pixel values above the clipping value.
 \textit{Creative Commons courtesy of anonymous.}}
\label{fig:clahe}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/clahe_before_after}
\caption{The upper left corner of position 43's in-focus-image recording at time $t=0$: before (left) and after (right) the application of CLAHE on the image.
 \textit{Creative Commons courtesy of anonymous.}}
\label{fig:clahe_before_after}
\end{figure}

\subsection{Subtraction by pixel-wise mean intensity}
\label{subsection:substraction_by_mean}

To substract an RGB image at time $M_t$ by pixel-wise mean intensity $\langle{M}\rangle$, the average intensity for each pixel across all time points is made for a position. For a stack with $n$ images, we can simply define the pixel-wise mean of the $i$-th channel for position $(x, y)$ as follows,

$$
\langle M \rangle_{x, y, i} := \frac{\sum_{1 \leq t \leq n} M_{t, x, y, i}}{n}
$$

For grayscale stack we define analogously,

$$
\langle G \rangle_{x, y} := \frac{\sum_{1 \leq t \leq n} G_{t, x, y, i}}{n}
$$

The pixel-wise substracted value of $i$-th channel is thus defined as point-wise operation for each position,

\begin{equation}
M^m_{t, x, y, i} := M_{t, x, y, i} - \langle M \rangle_{x, y, i}
\end{equation}

with the corresponding RGB value defined as,

$$
M^m_{t, x, y} := [M^m_{t, x, y, 1}; M^m_{t, x, y, 2}; M^m_{t, x, y, 3}]
$$

As for grayscale image we define following,

\begin{equation}
G^m_{t, x, y} := G_{t, x, y} - \langle G \rangle_{x, y}
\end{equation}

In the context of single-cell enclosure of micro-trench, this means that in the parts of images in which only dynamic movement of cells are observed, the pixel-wise mean intensity value of aforementioned parts will be very low. Subtracting every pixel the area from every time point with the average will barely affect the original intensity value.

On the other hand, applying the method on area with static objects such as micro-trench margin will reduce each pixel of the area with exactly the same intensity value as it barely changes during the experiment. This will nullify the static parts to large extent.

This method can be improved by iteratively repeating the calculation of $\langle M \rangle$. This will periodically static parts or static parts that abruptly moved (due to sudden shift in the well with regard to the camera for example). We call this algorithm \textbf{k-substraction by pixel-mean intensity}:

\begin{algorithm}[H]
 \KwData{$M_t$}
 \KwResult{$M^m_t$}
 $k$ \;
 \For{\forcond}{
 	calculates $\langle M \rangle$ from $M_t$ \;
 	$M^m_t = M_t - \langle M \rangle$ \;
 	$M_t = M^m_t$ \; 
 }

\caption{k-substraction by pixel-mean intensity algorithm}
\label{algorithm:k_mean_substraction}
\end{algorithm}

\subsection{Cell recognition}
\label{subsection:cell_recognition}

Due to the cells' rotund shape, the \textbf{blob detection} family of algorithm is well-suited for recognizing cells. In our pipeline, we use the so-called Laplacian of Gaussian (LoG) method.

\subsubsection*{Laplacian of Gaussian (LoG) detector}

Laplacian of Gaussian (LoG), also known as Marr-Hildreth-Opreator, is among the first and still the most popular method for detecting blob. It is characterized by applying the second derivative of $\sigma$-scaled Gaussian to detect a blob in an image.

To derive the method we first consider a 2-dimensional Gaussian kernel function,

\begin{equation}
G(x,y) = \frac{1}{{2\pi \sigma^2}} e^{-\frac{x^2 + y^2}{2 \sigma^2}}
\label{equation:gauss_2d_log}
\end{equation}

As the name suggest, the representation of LoG operator is obtained by applying the Laplace-Operator on the Gaussian,

$$
L(x, y) = \Delta G(x, y)
$$

This can be expanded into,

\begin{equation*}
\begin{aligned}
L(x, y) & = \frac{\partial^2 G(x, y)}{\partial x^2} + \frac{\partial^2 G(x, y)}{\partial y^2} \\ 
L(x, y) & = - \frac{1}{\pi \sigma^4} e^{-\frac{x^2 + y^2}{2\sigma^2}} \left( 1 - \frac{x^2 + y^2}{2 \sigma^2}\right)
\end{aligned}
\label{eq:svm_hard_margin}
\end{equation*}

Figure \ref{fig:pixdiff} shows the representation of LoG operator in 2D. Notice the inverse hat characteristic giving it the nickname \textbf{Mexican hat operator}.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/log_operator}
\caption{A LoG operator. Notice the lack of dimension in an operator as it transforms an image (in one space) into another image (still in the same space).}
\label{fig:pixdiff}
\end{figure}

The operator is then applied on two dimensional matrix of intensity image. To get maximum response from a blob, the zeros of the operator have to be aligned with the circle to detect \cite{marr1980theory}. Figure (TODO: show) shows an example of binary circle with diameter $r$ with the corresponding response from LoG operator on the surface of the circle.

%TODO continue

Applied to pre-processed out-of-focus image, LoG will return response with maxima in the center of cell sizes (see Figure TODO).

\subsection{Cell tracking}
\label{subsection:cell_tracking}

To track cell, the Linear Assignment Problem (LAP) tracker, by Jaqaman et al, 2018, is used. The method builds on the cell recognition method explained in Subsection \ref{subsection:cell_recognition}.

\subsubsection*{Linear Assignment Problem (LAP) framework of cell tracking}

The algorithm follows closely the case of linear assignment problem in bipartite graph. Given a bipartite graph $\mathcal{G}_b:= {\mathcal{G}_1, \mathcal{G}_2}$ and assignment cost function 
$c(v_i, v_j)$ for $v_i \in \mathcal{G}_1$ and $v_i \in \mathcal{G}_2$, find a bijection $f_{\mathcal{G}_b} : \mathcal{G}_1 \rightarrow \mathcal{G}_2$ so that the total cost function:

\begin{equation}
\sum_{v_i \in \mathcal{G}_1} c(v_i, f_{\mathcal{G}_b}(v_i))
\label{eq:lap_standard}
\end{equation}


is minimized. As seen in Equation \ref{eq:lap_standard}, the cost function and its corresponding constraint (bipartite assignment) is linear. Hence, the "linear" part of the assignment problem.

We can frame our cell tracking problem in a variant of linear assignment problem. Recall that, for each slice, a set of blobs representing cells are detected. To track cells, each blob in a slice has ot be connected with a blob in proceeding and succeeding slices (except in the case where a cell died).

Revisiting Subsection \ref{subsection:cell_recognition}, it is obvious that upon cell  detection the next step to track single cell movements in a stack is to assign to detected blobs $\mathcal{G_i}$ at time $t_i$ with the blobs detected at time $t_{i + 1}$, $\mathcal{G}_{i + 1}$ for $i \in [0, n - 1]$. To do that, first we define particle-to-particle cost function $c$. Jaqaman and his colleagues \cite{jaqaman2008robust} that, due to Brownian nature of cell movement, the square distance of blobs or its derivation sshould be used as cost function: 

\begin{equation}
d(v_i, v_j) = \| v_i - v_j\|^2_2
\label{eq:particle_cost_function}
\end{equation}

The algorithm is roughly divided into two main steps:

\begin{itemize}
\item  Creation of track segments from frame-to-frame blob linking.
\item  Gap closing and cell division inference to achieve closing of the track segments.
\end{itemize}

Both steps are framed as linear assigned problem. In the first step, two consecutive slices $M_i$  and $M_{i+1}$ are optimized for the links. To do that, a $(m + n) \times (n + m)$ matrix $\mathcal{C}$ is created, where $m$ and $n$ refer to the number of detected blobs in $M_u$ and $M_{i + 1}$ respectively. The matrix contains four quadrants:

\begin{itemize}
\item The upper left quadrant ($m \times n$ elements) contains the costs of linking blobs in $M_i$ to those in $M_{i + 1}$, also known as \textit{segment linking cost}.
\item The upper right quadrant ($m \times m$ elements) contains the costs of not linking blobs in $M_i$ to any blobs in $M_{i + 1}$, also known as \textit{segment stop cost}.
\item The lower left quadrant ($n \times n$ elements) contains the costs of of not linking blobs in $M_{i + 1}$ to any blobs in $M_i$, also known as \textit{segment start cost}.
\item The lower right quadrant ($n \times m$ elements) is auxiliary matrix used by LAP framework as formalism for its algorithm. The matrix is created by transposing the upper right left quadrant and replacing all non-infinity cost with minimal cost. 
\end{itemize}

The segment linking cost (upper left quadrant) is calculated as follows:

\begin{gather*}
c(v_k, v_l) =
\begin{cases}
  (d(v_k, v_l) \cdot \left(1 + \sum_{\mathbb{f}} \left(3 W_{\mathbb{f}} \frac{ f_{\mathbb{f}1} - f_{\mathbb{f}2} }{f_{\mathbb{f}1} + f_{\mathbb{f}2}} \right)\right)^2 & \text{if } d(v_k, v_l) \leq d_{max} \\    
  \infty & \text{else}  
\end{cases}
\end{gather*}

where $\mathbb{f}$ refers to each feature penalty and $W_{\mathbb{f}}$, $f_{\mathbb{f}1}$ and $f_{\mathbb{f}2}$ refer to the feature penalty factor, the value of feature $\mathbb{f}$ of $v_k$ and the value of feature feature $\mathbb{f}$ of $v_l$ respectively. Note that:

\begin{itemize}
\item If the distance is larger than the pre-defined maximum distance, then the link is forbidden. A blocking cost ($\infty$) is assigned between two blobs.
\item If no penalty feature is introduced then the cost is simply squared distance of two blobs.
\end{itemize}

The non-linking cost (upper right and lower left quadrants) is calculated as follows:

\begin{gather*}
c(v_k, v_l) =
\begin{cases}
  1.05 \cdot \mathbb{C} & \text{if } v_k = v_l \\
  \infty & \text{else}  
\end{cases}
\end{gather*}

where $\mathbb{C}$ is the maximum value of the upper left quadrant of the matrix.

The optimization over the matrix is then solved with Munkers \& Kuhn algorithm \cite{munkres1957algorithms}, which solves the problem in cubic time $\mathcal{O}(n^3)$. The algorithm returns assignment minimizing the sum of the costs.

The interpretation of the cost functions is thus: without any penalty, the optimization problem favors solution which minimizes the squared distance between two blobs. This is in line with previously mentioned assumption of Brownian motion of cells. By adding feature penalties, we aim at favoring linking bobs that are more similar to each other. In brute single particle linking problems, spots are generally all the same, and they only differ by position. However, there is a variety of problems for which these feature penalties can add robustness to the tracking process. In our case for example, the cells might pose several features that change over time depending on the treatment and cell-dependent characteristics (the phenomenon we further investigate using machine learning methods, see Subsections \ref{subsection:svm} and \ref{subsection:rf} and Section \ref{section:ml_results}).

Upon the linking of blobs into track segments, more refinement is then do to achieve globally optimal cell tracks configuration. To do this, three events are considered in our model:

\begin{itemize}
\item In case of \textit{gap closing} events, the end of a track segment is linked to the start of another track segment.
\item In case of \textit{splitting} events, the stat of one track segment is linked to non-terminal part of another track segment.
\end{itemize}

The matrix is created in similar fashion to the cost matrix in the first step, with the details best referenced directly to the article by Jaqaman et al \cite{jaqaman2008robust}. Also similar to the first part is the use of penalty features during segment creation. Unlike the the first part however, the segment merging cost is blocking ($\infty$) if the segments in question are separated by fames larger than pre-defined maximum number of frame gaps.

The optimal solution for the problem is again found by using Munkers \& Kuhn algorithm \cite{munkres1957algorithms}.

\subsection{Shift correction}

%TODO improve subsection

Now, consider a case in which images are shifted in a time-lapsed movie. TODO: explain mechanism. No rotation of camera is assumed, hence there are only two degree of freedoms (vertical and horizontal). Thus, a shift can be defined as a vector movement $\vec{v}$ of all points $x_{i,j} \in M_{t_i}$ in the time-lapse from time $t_i$ to $t_{i+1}$. Given two degrees of freedom and discreteness of the problem due to pixel representation, the task is reduced to finding difference in x- and y-axis ($\delta_x$ and $\delta_y$), so that the difference of transformed pixels at $t_i$ and $t_{i+1}$ are minimized, i.e.:

$$
argmin_{\delta_x, \delta_y} \{d(M_{t_i}, M_{t_{i+1}}^{\delta_x, \delta_y} + (\delta_x, \delta_y)^T)\}
$$

Where $M_{t_{i+1}}^{\delta_x, \delta_y}$ is the entries of matrix $M_{t_{i+1}}$ after applying the shift $\vec{v} := (\delta_x, \delta_y)^T$, i.e.

$$M_{t_{i+1}, \, x, \, y}^{\delta_x, \delta_y} = M_{t_{i+1}, \, x - \delta_x, \, y - \delta_y}$$

For the distance function $d$, the in all channels absolute difference function is used, which is defined as:

$$
d(M_i, M_j) = \sum_{c \, \in \, \{R, B, G\}} \sum_{x} \sum_{y} \vert M_{i, c, x, y} - M_{j, c, x, y} \vert
$$


Since some pixels are lost from the field of view during a shift, only a subset of subsequent images is used to determine the shift, preferably those around the center point. This will allow the largest search space possible, since the distance to all four margins of the image is maximized at the center point. The search for the optimal $(\delta_x, \delta_y)$ pair is implemented as a grid search along the x- and y-axis. An example of the search grid is shown in Figure \ref{fig:searchgrid}.\\

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/search_grid}
\caption{Search grid shift for Position 26. The search was conducted for shift between the last time point before and the first time point after the drugs treatment. The minimum is marked with thick black dot, which is returned after every grid-search call as inferred shift. In the position, the shift was inferred to be 8 pixels upwards and 5 pixels leftwards. Notice the repeating pattern of relatively favorable configurations after approximately 50 horizontal and 100 vertical pixels caused by lattice nature of the trenches.}
\label{fig:searchgrid}
\end{figure}

%TODO : RESTRUCTURE THE CHAPTER

Since the time-lapsed data consists mainly of grayscale image, the RGB encoding could be the directly transformed to grayscale encoding. Using the transformed method also speeds up the calculation process since the distance function only computes the difference of grayscale channel's values:

$$
d(M, N) =  \sum_{x} \sum_{y} \vert M_{c, x, y}^{gray} - N_{x, y}^{gray}\vert
$$

Due to lost pixels around the margin of before and after images, only the overlapping part of both slides are included after the correction. Thus, for an inferred shift of $(\delta_x, \delta_y)$, the new dimension of the images is then $(m - \delta_x) \times (n - \delta_y)$. This change would then propagation to the other time-lapse images to maintain consistency of the images.\\

Ideally, the shift correction should be done for each position to reduce the track dropout rate caused by image shifts. This is however computationally very expensive and, as seen in Figure \ref{fig:pixdiff}, not really necessary since the biggest shift indeed only happens right before and after the treatment, as it was expected during the experiment setting. As seen in Subsection \ref{subsection:cell_tracking}, the tracking allows certain amount of tolerance. In this regard, the other frame shifts are way within the tolerance of our tracking algorithm. As shown in Figure YY (TODO: add droput rate), the dropouts caused by frame shifts in the other time points are basically noisy dropout caused by random noise in time-lapse movie being tracked as cells \cite{jaqaman2008robust}.

The algorithm for shift inference is available in Appendix \ref{appendix:algo}.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/pixdiff}
\caption{Pixel difference between consecutive frames at Position 26. In most cases, the pixel difference between the frames is mainly caused by moving cells. The diference during the treatment, on the other hand, is caused by physical shift of the frame. While moving cells mostly caused minimum noise-like pixel difference, the physical shift of field of view distorts the physical alignment and evokes immense pixel difference.}
\label{fig:pixdiff}
\end{figure}

\subsection{Cell death signal determination}

As mentioned in previous parts, two cell death signals are deployed in this experiment: the PI and Caspase 3/7. As mentioned in 

%TODO continue


%TODO rework subsection
%TODO move to review section of Summary and Outlook

\subsection{Support vector machine (SVM)}
\label{subsection:svm}

In machine learning, an SVM is a construct which, given training set $\mathbf{S} \subset \mathbf{D}$ with,

$$
\mathbf{S} = \{\mathbf{x_1}, \cdots, \mathbf{x}_{\|\mathbb{S}\|}\}
$$

and corresponding target class,

$$
\mathbf{T} = \{y_1, \cdots, y_{\|\mathbb{S}\|}\}
$$

finds following things,

\begin{itemize}
\item A hyperplane that separates the input by its class, so that every point belonging to one class is located on one side of the hyperplane. This hyperplane is, in turn, defined by,
\item support vectors.
\end{itemize}

A hyperplane is defined as set of points $\mathbf{x}$ in $\mathbf{S}$ satisfying following criteria,

\begin{equation}
\mathbf{w} \cdot \mathbf{x} - b = 0
\label{equation:hyperplane}
\end{equation}

(See Figure \ref{fig:svm}) where $\mathbf{w}$ and $\frac{b}{\| \mathbf{w} \|}$ denote the normal vector to the hyperplane and the distance of the hyperplane from the origin along the normal vector $\mathbf{w}$. For every class $c_i$, the set of of data points satisfyling criteria,

\begin{equation}
\mathbf{w} \cdot \mathbf{x} - b = c_i
\label{equation:support_vector}
\end{equation}

are called support vectors. The distance from the hyperplane to support vectors is thus,

$$\frac{1}{\| \mathbf{w} \|}$$

For two-classes classification, the classes are conventionally annotated as $-1$ and $1$. As the Equations \label{equation:support_vector} suggests, for every point beyond (seen from the perspective of hyperplane) the support vectors of the class $c = -1$, following unequality applies,

\begin{equation}
\mathbf{w} \cdot \mathbf{x} - b < -1
\label{equation:svm_class_1}
\end{equation}

the analogous applies to the class $c = 1$,

\begin{equation}
\mathbf{w} \cdot \mathbf{x} - b > 1
\label{equation:svm_class_2}
\end{equation}

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/svm}
\caption{Illustration of support vector machine in 2D. For black class there is one hyperplane-definining point (marked with grey margin) while for white class there are two (marked with bold black margin). The support vector $\mathbf{w}$ is maximized for each class.}
\label{fig:svm}
\end{figure}

Following large margin principle \cite{vapnik1964note, murphy2012machine}, an SVM tries to find support vectors that maximize $\| \mathbf{w} \|$. Given separability of the training data, the support vector $\mathbf{w}$ can then be solved by solving following optimizing problem, 

\begin{equation*}
\begin{aligned}
& \underset{\mathbf{
w}}{\text{minimize}}
& & \| \mathbf{w}\| \\
& \text{subject to}
& & y_i(\mathbf{w} \cdot \mathbf{x}_i - b \leq 1) \text{ for } i = 1, \cdots \| \mathbf{S} \|.
\end{aligned}
\label{eq:svm_hard_margin}
\end{equation*}

This hard-margin only converges only when the data are linearly separable in mapped space (also known as \textbf{feature space}). This is especially bad since many problems are not linearly separable in their original space. There are two fundamentals way of relaxing this problem to enable classification using SVM:

\begin{itemize}
\item Relaxation of the definition of SVM by allowing data points to be misclassified.
\item Blowing up the input space into sufficiently high dimensional features using kernel trick.
\end{itemize}

\subsubsection*{Relaxation of SVM}

A relaxation of above explained problem is known as soft margin SVM or $\xi$-SVM \cite{cortes1995support}. The problem allows misclassification of some training data. In SVM, misclassification occurs when a data point belonging to a certain class $c_j$ is located \textbf{not} in the area defined by the margin $\mathbf{w} x_{c_j} = c_j$ for class $c_j$.

The problem is thus reduced to minimizing following term,

\begin{equation}
\left[\frac{1}{n} \sum_{i=1}^{\|\mathbf{S}\|} \max\left(0, 1 - y_i(\mathbf{w} \cdot \mathbf{x_i} - b)\right) \right] + \lambda\| w \|^2
\label{eq:soft_margin_svm_term}
\end{equation}

The term inside of summation is called \textbf{classification error}. For correctly classified class we have $\mathbf{w} \cdot \mathbf{x}_i - b \leq -1$ and $\mathbf{w} \cdot \mathbf{x}_i - b \geq 1$ for $c_i = -1$ and $c_i = 1$ respectively, i.e. the summation term is $0$ for every correctly classified data point. The coefficient $\lambda$ is regularization coefficient which penalizes the magnitude of normal vector $\mathbf{w}$. Note that the higher the dimension of $\mathbf{w}$ the larger the penalty is. This is important for next part on \textbf{kernel methods}.

Minimizing Term \ref{eq:soft_margin_svm_term} is equal to optimizable with differentiable objective function \cite{nocedal2006numerical}. We can for example introduce a variable $\zeta_i$ defined as,

\begin{equation}
\zeta_i = \max(0, 1 - y_i(\mathbf{w} \cdot \mathbf{x}_i - b))
\label{eq:svm_zeta}
\end{equation}

This can be written as $y_i(\mathbf{w} \cdot \mathbf{x}_i - b) \geq 1 - \zeta_i$. Geometrically this is the maximum distance of a wrongly classified data point from the support vector. Using Definition \ref{eq:svm_zeta} we can reduce optimization posed in Term \ref{eq:soft_margin_svm_term} to,

\begin{equation*}
\begin{aligned}
& \underset{\mathbf{
w}}{\text{minimize}}
& & \frac{1}{\| \mathbf{S} \|} \sum_{i=1}^{\| \mathbf{S} \|} \zeta_i + \lambda \| \mathbf{w} \|^2 \\
& \text{subject to}
& & y_i (\mathbf{w} \cdot \mathbf{x}_i - b) \leq 1 - \zeta_i \\
& \text{and} && \zeta_i \geq 0 \text{ for all } i.
\end{aligned}
\end{equation*}

\subsubsection*{Kernel method}

As mentioned before, the optimization problem posed Subsection \ref{eq:svm_hard_margin} converges only in the case of linear separability of training data. While this mostly is not the case, Vapnik and Cortes \cite{cortes1995support} proposed the so-called \textbf{kernel trick}. It utilizes a kernel function $\Phi$ which maps the training set into high dimensional space representation. Essentially, a kernel function $\Phi: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}^+$ is a symmetric and non-negative function following the criteria by Mercer \cite{mercer1909functions} defining it as, among others, general measure of similarity between two vectors.

In our case, we focus on two very well-known examples of kernel function, \textbf{the polynomial kernel function of order n}, defined as,

\begin{equation}
\Phi_{RBF}(\mathbf{x}_i, \mathbf{x}_j) = \left(\mathbf{x}_i^T \mathbf{x}_j + r \right)^n \text{ with } r > 0
\label{eq:svm_rbf}
\end{equation} 

and the \textbf{radial basis function (RBF)}, defined as,

\begin{equation}
\Phi_{RBF}(\mathbf{x}_i, \mathbf{x}_j) = exp\left(\frac{\| \mathbf{x}_i - \mathbf{x}_j \|^2}{2 \sigma^2}\right)
\label{eq:svm_rbf}
\end{equation}

In both examples we can observe the assumed dimensionality of target feature space. A polynomial kernel function of order n maps the training data to $n$-dimensional feature  space. Thus, blowing up the training data into higher dimensional requires the explicit assignment of a very high $n$ value. The  radial basis function on the other hand does not assume any dimensionality (or rather, it assumes \textit{unbounded} dimensionality) as the Gaussian function used only assumes the input vectors $\mathbf{x}_i$ and $\mathbf{x}_j$ to be of the same dimensionality. 

While assuming very high/unbounded dimensionality seems convenient at the start, this is not very straightforward, since:

\begin{itemize}
\item Given a non-powerful kernel function $\Phi$ that only maps the input into bounded number of dimension, the optimization problem won't converge.
\item Given a powerful kernel function $\Phi$ capable of blowing up the dimension to very high dimensions (see the part about kernel function later on) the kernel might settle for unnecessarily sparse dimensional reducing the generalizability of the problem.
\end{itemize}

\subsubsection*{Regularization}

One solution is too introduce regularization. As already shown in Term \ref{eq:soft_margin_svm_term}, we can add a regularization term such as, 

\begin{itemize}
\item $L_1$-regularization: $\lambda \| \mathbf{w} \|^2$ \cite{park2007l1}, and
\item $L_2$-regularization (Tikhonov regularization): $\lambda \| \mathbf{w} \|$ \cite{tikhonov1977solutions}.
\end{itemize}

Adding regularization will control against overfitted model by penalizing higher dimensional hyperplane $\mathbf{w}$ as it the error will get blown up the higher dimensional, and its error minimizing hyperplane, are chosen.

\subsection{Random Forests (RF)}
\label{subsection:rf}

\chapter{Analytic Pipeline}

%TODO adjust figure references
%TODO expand chapter

In this chapter the image and data analytic pipeline is presented. Each pipeline of image processing analysis is elaborated with reference to publications and the definitions from Chapter \ref{chapter:data_and_method}.

\section{Image computing}

To assess single cell characteristics (like its lifetime, time-to-death, division time, daughter cells and other information relating to its time- and generation-dependent cell cycle information), we track all cells in the brightfield channel from the start of the movie, assign cells to individual micro-trenches to e.g. filter out micro-trenches with multiple starting cells, and determine cell death via marker onset in the fluorescent channels.

Figure \ref{fig:pipeline1}a shows how the image is sequentially processed from out-of-focus image to cell trees information.

\section{Single cell tracking in the brightfield channel}

To identify and track single cells from movie start, each out-of-focus brightfield image is processed in the following way (see Figure \ref{fig:pipeline1}a for pipeline visualization): 

\begin{itemize}
\item First, a Gaussian blur (Zuiderveld, 1994 \cite{zuiderveld1994contrast} and also Subsection \ref{subsection:gaussian_blur} of Data Methods chapter) with a large radius of 50 pixel is applied to each brightfield image (Figure \ref{fig:pipeline1}b) to estimate its individual background and identify e.g. gradients at the edges of the imaged area (Figure \ref{fig:pipeline1}c). Afterwards, we correct the original image by dividing through this background image (Figure \ref{fig:pipeline1}d)

\item Then, we normalize the local contrast of each image in one position (Kittler and Illingworth, 1986 \cite{kittler1986minimum} and Subsection \ref{subsection:clahe}) to separate foreground from background (Figure \ref{fig:pipeline1}e)..

\item To reduce noise from micro-trench margins in the foreground (Figure \ref{fig:pipeline1}e and Subsection \ref{subsection:substraction_by_mean}), we calculate the pixel-wise intensity average from all timepoints of one position (Figure \ref{fig:pipeline1}f) and subtract this from each image to generate a binary image with mainly cells in the foreground (Figure \ref{fig:pipeline1}g). This creates a mask of the cells..

\item Therein, single cells are recognized using the Laplacian of Gaussian detector in the Fiji TrackMate plugin \cite{tinevez2017trackmate} (Figure \ref{fig:pipeline1}h and Subsection \ref{subsection:cell_recognition}) with a blob diameter of 15 inches and a downsampling factor of 2. See Subsection \ref{subsection:cell_recognition} of Data and Methods chapter for more details on the method.

\item The detected cells are concatenated to tracks (Figure \ref{fig:pipeline1}i and Subsection \ref{subsection:cell_tracking}) and cellular trees using the Linear Assignment Problem (LAP) tracker\cite{jaqaman2008robust} in TrackMate with a maximal frame-to-frame linking distance of 25 inches, a maximal track segment gap closing distance of 35 inches, a maximal track segment gap closing of 4 frames, and a maximal track segment splitting distance of 25 inches.
\end{itemize}

\section{Assignment of cells to individual micro-trenches}

%TODO add references
%TODO correct wording

To assign individual micro-trenches to tracked single cells and emerging clones, we generate masks for each micro-trench. First, we adjust brightness and contrast manually for one position so that only the margin of the micro-trenches are visible (Fig. S1k). Then, Robust Automatic Threshold Selection (RATS) is applied on each adjusted brightfield image, which creates a binary image of the micro-trench margins (Fig. S1l). These margins are then filled to create micro-trench masks using fill holes command in Fiji (Fig. S1m). Each mask (appearing as a black rod-like form) is assigned an unique identity, which is then used as an identifier for tracks in both the brightfield and the fluorescent channel for filtering of cells and clones.

\section{Single cell tracking in the fluorescent channel}

%TODO add references
%TODO correct wording

Caspase and PI are imaged in the red (PI) and green (Caspase) fluorescent channel and used as indicator of cell death. Dying cells stop moving, become small and unstructured, and loose a distinctive bright signal in the brightfield channel, which makes them hard to track. We thus track dying cells in the fluorescent channels and concatenate the tracks with earlier tracks in the brightfield to determine time-to-death. To track cells in the fluorescent channel, we apply the following pipeline (see Fig. S2a): We first adjust brightness and contrast manually (Fig S2b) followed by mean correction of static noises by subtracting each imagewith average intensity over all imagess (Fig S2c and Fig S2d). The detection and tracking part of image computing pipeline is identical (Fig S2 subplot A). Eventually, death time of a cell is indicated by the onset of fluorescence track being recognized in red or green channel, depending on treatment:or Vincristine treatment and control, the cell death is based on tracks in red (PI) channel while the cell death in Daunorubicin treatment is based on green (Caspase) channel.

\section{Pipeline visualization}

The image processing pipelines for both brightfield and florescence channels can be seen in following figures:

%TODO complete

\begin{landscape}
\begin{figure}[H]
   \centering
    %\includegraphics[width=1.0\textwidth]{Image.eps}
    \includegraphics[height=0.95\textheight]{images/pipeline/01.pdf}
    \caption{Brightfield channel pipeline}
\end{figure}
\label{fig:pipeline1}
\end{landscape}

\begin{landscape}
\begin{figure}[H]
   \centering
    %\includegraphics[width=0.5\textwidth]{Image.eps}
    \includegraphics[height=\textheight]{images/pipeline/02.pdf}
    \caption{Fluoroscence channel pipeline}
\end{figure}
\label{fig:pipeline2}
\end{landscape}

\section{Implementation and availability}

The image and tracking data is combined and post-processed using Python scripts utilizing OpenCV\cite{bradski2008learning}, NumPy \cite{walt2011numpy}, pandas \cite{mckinney2010data} and Matplotlib \cite{hunter2007matplotlib}, and the Jython and Trackmate \cite{tinevez2017trackmate, pedroni2002jython} plugin in Fiji \cite{schindelin2012fiji}. The Python package scikit-learn is used for training and testing both support vector machine and random forest \cite{scikit-learn}. Our code is available at:

\href{https://github.com/raharjaliu/MA/tree/master/source}{https://github.com/raharjaliu/MA/tree/master/source}.

\chapter{Results}

%TODO complete

\begin{verbatim}
I. Pipeline

II. Quantitative Analysis
\end{verbatim}

\section{Quantitative analysis}

\section{Machine learning models}
\label{section:ml_results}

\chapter{Summary and Outlook}

%TODO complete

\section{Fully automated microfluidics pipeline}

\section{Fully automated image analysis pipeline}

Improvements in analysis pipeline:

\begin{itemize}
\item better method for masking recognition
\item possibly even machine learning based methods
\end{itemize}

\begin{verbatim}
I. Summary (and discussion?)
Connect Results with Background

II. Outlook
Improvemnt capability
\end{verbatim}

\subsection{Automated micro-trench masking}

One positive thing about the design of our data analysis pipeline is the modularity of its component. As mentioned multiple times in this thesis (in particular in Subsection \ref{subsection:micro_trench_masking}, our method can well be improved by automatizing the micro-trench masking process. Although itself not a bottleneck of the process, as the entire dataset of 64 positions could be processed in less than an hour, automating this process will save time and more importantly, clicking and draging efforts, thus lets scientists do the things that matter more for them.

There are several methods that came to mind in improving the micro-trench masking, which generally can be separated in two groups: ML-based methods and non-ML-based methods. ML-based methods generally require either a manual annotation of correctly recognized micro-trench (a class of methods called supervised learning methods) or certain prior knowledge regarding distinctive features of micro-trench that separate it from background image and/or the cells (a class of methods called unsupervised/semi-supervised learning methods).

In the following part of this subsection various potential implementations of both ML-based and non-ML-based methods are listed down.

\subsubsection*{Maximally interesting extremal region (MNER)}

Introduced by Oliver Hilsenbeck in his master's thesis in 2014 \cite{hilsenbeck2014maximally} %TODO continue


%TODO

%% BIBLIOGRAPHY %%

\bibliographystyle{unsrt}
\bibliography{mabib}

%% APPENDICES %%

\begin{appendices}

\chapter{Experiment Protocols}

\section{Micro-trenches array fabrication}
\label{appendix:microtrench}

\subsection*{Photolithography of the SU-8 wafer}

The fabrication of the SU-8 (MicroChem Corp, USA) wafer was executed in a in-house cleanroom facility using a ProtoLaser LDI system (LPKF Laser \& Electronika, Naklo, Slovenia), with a 375 nm wavelength laser and 1 μm spot diameter.

\subsection*{Softlithography and micromolding}

Polydimethylsiloxane (PDMS) prepolymer solution is mixed with the crosslinker in a 10:1 ratio (w/w) (Sylgard 184, Dow Corning, USA) and then degassed under vacuum. PDMS is then purred on the SU-8 wafer, degassed and cured in 50 oC. The resulting PDMS stamp is peeled off the wafer and cut into appropriate shapes. The PDMS pieces, with 25 1/4 pillars in height, are activated with argon plasma and then immediately placed upside down on a silanized with TMSPMA (3-(Trimethoxysilyl)propyl methacrylate, Sigma-Aldrich) glass coverslip. A solution of PEG-DA (Mn=258) containing 2\% v/v of the 2-hydroxy-2methylpropiophenone (both from Sigma-Aldrich, Germany) is freshly prepared and then a drop is deposited at the edge of the PDMS stamp. The PDMS stamp is filled by capillary force induced flow. PEG-DA is then polymerized in an UV-ozone cleaning system (UVOH 150 LAB, FHR, Ottendorf, Germany). Next, the PDMS stamps are peeled off and the resulting micro-trenches of cross-linked PEG-DA are dried in an oven (Binder GmbH, Tuttlingen, Germany) overnight at 50oC. Finally, the slides are sonicated with 70\% ethanol and distilled water before a sticky slide is attached on top (8-well sticky slide, ibidi GmbH, Munich, Germany).

\section{Images acquisition}
\label{appendix:imageacquisition}

%TODO add example picure setting

Imaging was performed under an inverted Nikon Ti Eclipse microscope with a motorized stage (Tango XY Stage Controller, M\"arzh\"auser Wetzlar GmbH \& Co. KG, Germany), a CFI Plan Fluor DL 10X objective, a pco.edge 4.2 Camera (PCO AG, Kelheim, Germany) and a Lumencor Sprectra LED fluorescence lamp. For detection of the Caspase-3/7 and the PI marker, the following filters were used respectively, 474/27 nm, 554/23 (excitation) and 515/35 nm, 595/35 nm (emission). Brightfield out of focus (-20 $\mu$m) images were taken every 10 minutes and in-focus  brightfield and fluorescence images every 30 minutes for 48 hours. Vincristine or Daunorubicin were added after 20 hours from the beginning of the imaging. During the recording samples were kept at a constant temperature of 37\degree C and CO2 using an Okolab heating and CO\textsubscript{2} 2 box (OKOLAB S.R.L., NA, Italy). For the synchronized population, the double thymidine block protocol was followed. Briefly, MOLM-13s cells at the exponentially growing phase were incubated in blocking medium (culture medium supplemented with 2 mM Thymidine (CAS 50-89-5, Calbiochem\textsuperscript{\textregistered}, Germany)) for 24 hours. Cells were then released and incubated in culture medium for 8 hours and finally were incubated in blocking medium for 12 hours. After 2 hours, the synchronized population was seeded in the slide bearing the micro-trenches together with the markers and drugs at the same conditions as the unsynchronized population, and imaged for 24 hours.

\chapter{Algorithms}
\label{appendix:algo}

The algorithms are, unless specified written in pseudo-code.

\section*{Shift inference} 

Following is the algorithm to infer shift between slice written in Python:

\begin{verbatim}
def infer_shift(last_slide, first_slide, search_space=(200, 200)):
    
  if (search_space[0] % 2 != 0) or (search_space[1] % 2 != 0):
    print("Search spaces have to be even!")
    return None
  else:
    
    ## calculate absolute difference for various shifts
    x1 = search_space[0]
    x2 = search_space[0]
    y1 = search_space[1]
    y2 = search_space[1]
    mid = f1.shape[0] // 2, f1.shape[1] // 2
        
    ## results storage        
    absdiffs = np.zeros((search_space[0] + 1, search_space[1] + 1))

    ## last slide before treatment
    f1sub = f1[(mid[0] - x1):(mid[0] + x2), (mid[1] - y1):(mid[1] + y2)]
    
    ## search space 
    xdiff1 = -int(search_space[0] / 2)
    xdiff2 = int(search_space[0] / 2) + 1
    ydiff1 = -int(search_space[1] / 2)
    ydiff2 = int(search_space[1] / 2) + 1
    
    for xdiff in range(xdiff1, xdiff2):
      for ydiff in range(ydiff1, ydiff2):
      
        ## calculate absolute difference for shift
        f2sub = f2[(mid[0] - x1 + xdiff):(mid[0] + x2 + xdiff), 
                   (mid[1] - y1 + ydiff):(mid[1] + y2 + ydiff)]
        x = xdiff + int(search_space[0] / 2)
        y = ydiff + int(search_space[1] / 2)
        absdiff_xy = np.sum(cv2.absdiff(f1sub, f2sub).ravel())
        absdiffs[x][y] = absdiff_xy
        
        
    ## calculate shift based on calibration data
    x = np.argmin(absdiffs) // absdiffs.shape[0]
    y = np.argmin(absdiffs) % absdiffs.shape[0]
    
    """
    True shift is the opposite of coordinate encoded
    in absdiff
    
    Let X2 the second image and X1 the first image.
    If the sub-image of first slide of X2 centered
    at (c1 + s1, c2 + s2) fits the most with the sub-image
    of the last slide of X1 centered at (c1, c2)
    then the images shift by (-s1, -s2) upon treatment
    """
    diff = -(x - search_space[0] / 2), -(y - search_space[1] / 2)

    return diff
\end{verbatim}

\end{appendices}

\end{document}